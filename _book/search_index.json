[["index.html", "Creating the flowkernel R package 1 Preliminaries 1.1 DESCRIPTION file 1.2 Package-level documentation", " Creating the flowkernel R package Jacob Bien, Farhad de Sousa 2024-06-24 1 Preliminaries This document uses litr to define the flowkernel R package. When the index.Rmd file is rendered, the R package is created along with the bookdown you are reading. To do so in RStudio, you can simply open index.Rmd and press “Knit” to render the bookdown (and open _book/index.html to see the result). More generally, in a console you can run the following: litr::render(&quot;index.Rmd&quot;) 1.1 DESCRIPTION file We start by specifying some basic information for the description file: usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;Smoothly Varying Mixture of Gaussians Modeling&quot;, Description = &quot;This package uses kernel-smoothed EM to estimate a smoothly varying mixture of Gaussians model.&quot;, `Authors@R` = person( given = &quot;Jacob&quot;, family = &quot;Bien&quot;, email = &quot;jbien@usc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;F. Last&quot;) 1.2 Package-level documentation Let’s include some package-level documentation. Besides being user-friendly, it’s also needed because we’ll be using “import from” later. Also, notice that we are importing all of mclust. This is because of this error involving mclustBIC(), which is called when we call Mclust(). #&#39; Smoothly Varying Mixture of Gaussians Modeling #&#39; #&#39; This package uses kernel-smoothed EM to estimate a smoothly varying mixture of Gaussians model. #&#39; #&#39; @docType package #&#39; @import mclust "],["the-model.html", "2 The model 2.1 Generating data from model 2.2 Visualizing the raw data: 2.3 Visualizing data and model", " 2 The model Our interest is in modeling a sequence of scatter plots measured over time. That is, we observe \\(Y_{it}\\in\\mathbb R^d\\) for \\(i=1,\\ldots,n_t\\) and \\(t=1,\\ldots,T\\). In continuous-time flow cytometry data, we notice that this data has two properties: Each scatter plot looks approximately like a mixture of Gaussians. The general clustering structure seen in each scatter plot is slowly varying over time. To model data like this, we wish to fit a smoothly-varying mixture of Gaussians model: \\[ Y_{it}|\\{Z_{it}=k\\}\\sim N_d(\\mu_{kt},\\Sigma_{kt})\\qquad\\mathbb P(Z_{ik}=k)=\\pi_{kt} \\] where \\((\\mu_{kt},\\Sigma_{kt},\\pi_{kt})\\) are slowly varying parameters. It will be useful to have data generated from this model for testing purposes, so we begin by defining a function for simulating from this model. 2.1 Generating data from model #&#39; Generate data from smoothly-varying mixture of Gaussians model #&#39; #&#39; The smoothly-varying mixture of Gaussians model is defined as follows: #&#39; #&#39; At time t there are n_t points generated as follows: #&#39; #&#39; Y_{it}|\\{Z_{it}=k\\} ~ N_d(mu_{kt},Sigma_{kt}) #&#39; where #&#39; P(Z_{ik}=k)=pi_{kt} #&#39; and the parameters (mu_t, Sigma_t, pi_t) are all slowly varying in time. #&#39; #&#39; This function generates Y and Z. #&#39; #&#39; @param mu_function a function that maps a vector of times to a T-by-K-by-d #&#39; array of means #&#39; @param Sigma_function a function that maps a vector of times to a #&#39; T-K-by-d-by-d array of covariance matrices #&#39; @param pi_function a function that maps a vector of times to a T-by-K vector #&#39; of probabilities #&#39; @param num_points a T vector of integers giving the number of points n_t to #&#39; generate at each time point t. #&#39; @export generate_smooth_gauss_mix &lt;- function(mu_function, Sigma_function, pi_function, num_points) { times &lt;- seq_along(num_points) mu &lt;- mu_function(times) Sigma &lt;- Sigma_function(times) pi &lt;- pi_function(times) K &lt;- ncol(pi) # number of components d &lt;- dim(mu)[3] dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) z &lt;- list() # z[[t]][i] = class of point i at time t y &lt;- list() # y[[t]][i,] = d-vector of point i at time t for (t in times) { z[[t]] &lt;- apply(stats::rmultinom(num_points[t], 1, pi[t, ]) == 1, 2, which) y[[t]] &lt;- matrix(NA, num_points[t], d) for (k in 1:K) { ii &lt;- z[[t]] == k # index of points in component k at time t if (sum(ii) == 0) next if (d == 1) y[[t]][ii, ] &lt;- stats::rnorm(n = sum(ii), mean = mu[t, k, ], sd = Sigma[t, k, , ]) else y[[t]][ii, ] &lt;- mvtnorm::rmvnorm(n = sum(ii), mean = mu[t, k, ], sigma = Sigma[t, k, , ]) } } list(y = y, z = z, mu = mu, Sigma = Sigma, pi = pi) } We have used two packages in this function, so let’s add these into our package. usethis::use_package(&quot;stats&quot;) usethis::use_package(&quot;mvtnorm&quot;) ## ✔ Adding &#39;stats&#39; to Imports field in DESCRIPTION ## • Refer to functions with `stats::fun()` ## ✔ Adding &#39;mvtnorm&#39; to Imports field in DESCRIPTION ## • Refer to functions with `mvtnorm::fun()` Let’s generate simple examples in the \\(d=1\\) and \\(d=3\\)cases: set.seed(123) d &lt;- 1; K &lt;- 2; ntimes &lt;- 200 ex1 &lt;- list( mu_function = function(times) { mu &lt;- array(NA, c(ntimes, K, d)) mu[, , 1] &lt;- cbind(sin(2 * pi * times / 30), 2) mu }, Sigma_function = function(times) { Sigma &lt;- array(NA, c(ntimes, K, 1, 1)) Sigma[, , 1, 1] &lt;- 0.25 Sigma }, pi_function = function(times) { pi1 &lt;- seq(0.2, 0.8, length=length(times)) cbind(pi1, 1 - pi1) }, num_points = rep(40, ntimes) ) ex1$dat &lt;- generate_smooth_gauss_mix(ex1$mu_function, ex1$Sigma_function, ex1$pi_function, ex1$num_points) d &lt;- 3; K &lt;- 4; ntimes &lt;- 200 ex2 = list( mu_function = function(times) { mu &lt;- array(NA, c(ntimes, K, d)) mu[, , 1] &lt;- cbind(0.5*cos(2 * pi * times / 30), 0.3*sin(2 * pi * times / 30), sin(2 * pi * times / 30), -3) mu[, , 2] = cbind (0.3*sin(2 * pi * times / 30), 2, -1, 0.6*cos(2 * pi * times / 30)) mu[, , 3] = cbind(2, 0.7*cos(2 * pi * times / 30), 0.4*sin(2 * pi * times / 30), 1) mu }, Sigma_function = function(times) { Sigma &lt;- array(NA, c(ntimes, K, d, d)) Sigma[, , 1, 1] &lt;- 0.10 Sigma[, , 1, 2] &lt;- 0 Sigma[, , 1, 3] &lt;- 0 Sigma[, , 2, 1] &lt;- 0 Sigma[, , 2, 2] &lt;- 0.10 Sigma[, , 2, 3] &lt;- 0 Sigma[, , 3, 1] &lt;- 0 Sigma[, , 3, 2] &lt;- 0 Sigma[, , 3, 3] &lt;- 0.10 Sigma }, pi_function = function(times) { pi1 &lt;- seq(0.2, 0.3, length=length(times)) cbind(pi1, pi1, 2*pi1/3, 1- (2*pi1 + 2*pi1/3)) }, num_points = rep(150, ntimes) ) ex2$dat &lt;- generate_smooth_gauss_mix(ex2$mu_function, ex2$Sigma_function, ex2$pi_function, ex2$num_points) 2.2 Visualizing the raw data: Let’s make a function for visualizing the data in the one-dimensional and three dimensional cases. library(magrittr) # we&#39;ll be using the pipe in this document The function will take as input the following argument: ###&quot;y-param&quot;### #&#39; @param y length T list with `y[[t]]` being a n_t-by-d matrix We define this bit of documentation in its own code chunk so that it can be easily reused since multiple functions in the package take it as input. #&#39; Plot raw data #&#39; &lt;&lt;y-param&gt;&gt; #&#39; #&#39; @export plot_data &lt;- function(y) { d &lt;- ncol(y[[1]]) if (d == 1){ fig &lt;- purrr::map_dfr(y, ~ tibble::tibble(y = .x), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) %&gt;% ggplot2::ggplot(ggplot2::aes(x = .data$time, y = .data$y)) + ggplot2::geom_point(alpha = 0.2) } else if (d == 3){ d &lt;- ncol(y[[1]]) max_val &lt;- list() max_val_time &lt;- list() min_val = list() min_val_time = list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y = unname(y) y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) y$time = as.integer(y$time) fig &lt;- plotly::plot_ly( data = y, x = ~x, y = ~y, z = ~z, type = &quot;scatter3d&quot;, frame = ~time, mode = &quot;markers&quot;, size = 80, colors = colorRamp(c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;cyan&quot;, &quot;magenta&quot;, &quot;brown&quot;, &quot;gray&quot;, &quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;darkorange&quot;))) %&gt;% plotly::layout(title = &#39;Raw Data&#39;, scene = list( xaxis = list(range = c(1.1 * min_val_time[[1]], 1.1 * max_val_time[[1]]), title = &#39;diam_mid&#39;), yaxis = list(range = c(1.1 * min_val_time[[2]], 1.1 * max_val_time[[2]]), title = &#39;Chl_small&#39;), zaxis = list(range = c(1.1 * min_val_time[[3]], 1.1 * max_val_time[[3]]), title = &#39;pe&#39;), aspectmode = &quot;manual&quot;, aspectratio = list(x = 1, y = 1, z = 1) # Specify the fixed aspect ratio )) } return(fig) } We’ve used some functions from other packages, so let’s include those in our package: usethis::use_pipe() usethis::use_package(&quot;purrr&quot;) usethis::use_package(&quot;tibble&quot;) usethis::use_package(&quot;dplyr&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_import_from(&quot;rlang&quot;, &quot;.data&quot;) usethis::use_package(&quot;plotly&quot;) usethis::use_package(&quot;grDevices&quot;) ## ✔ Adding &#39;magrittr&#39; to Imports field in DESCRIPTION ## ✔ Writing &#39;R/utils-pipe.R&#39; ## • Run `devtools::document()` to update &#39;NAMESPACE&#39; ## ✔ Adding &#39;purrr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `purrr::fun()` ## ✔ Adding &#39;tibble&#39; to Imports field in DESCRIPTION ## • Refer to functions with `tibble::fun()` ## ✔ Adding &#39;dplyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `dplyr::fun()` ## ✔ Adding &#39;ggplot2&#39; to Imports field in DESCRIPTION ## • Refer to functions with `ggplot2::fun()` ## ✔ Adding &#39;rlang&#39; to Imports field in DESCRIPTION ## ✔ Adding &#39;@importFrom rlang .data&#39; to &#39;R/flowkernel-package.R&#39; ## ✔ Writing &#39;NAMESPACE&#39; ## ✔ Adding &#39;plotly&#39; to Imports field in DESCRIPTION ## • Refer to functions with `plotly::fun()` ## ✔ Adding &#39;grDevices&#39; to Imports field in DESCRIPTION ## • Refer to functions with `grDevices::fun()` Let’s look at our two examples using this plotting function: plot_data(ex1$dat$y) plot_data(ex2$dat$y) 2.3 Visualizing data and model We’ll also want a function for plotting the data with points colored by true (or estimated) cluster. And it will be convenient to also be able to superimpose the true (or estimated) means. The next function does this: #&#39; Plot data colored by cluster assignment with cluster means #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param z a length T list with `z[[t]]` being a n_t vector of cluster assignments #&#39; @param mu a T-by-K-by-d array of means #&#39; @export plot_data_and_model &lt;- function(y, z, mu) { d &lt;- ncol(y[[1]]) K &lt;- ncol(mu) ntimes = length(z) if (d == 1){ dat_df &lt;- purrr::map2_dfr(z, y, ~ tibble::tibble(z = as.factor(.x), y = .y), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) means_df &lt;- tibble::as_tibble(mu[, , 1]) %&gt;% dplyr::mutate(time = dplyr::row_number()) %&gt;% tidyr::pivot_longer(-.data$time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) fig &lt;- ggplot2::ggplot() + ggplot2::geom_point( data = dat_df, ggplot2::aes(x = .data$time, y = .data$y, color = .data$z), alpha = 0.2 ) + ggplot2::geom_line( data = means_df, ggplot2::aes(x = .data$time, y = .data$mean, group = .data$cluster) ) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Cell Diameter&quot;) # Label the x-axis and y-axis } else if (d == 3) { K &lt;- ncol(mu) d &lt;- ncol(y[[1]]) z_dat &lt;- unlist(z) ntimes = length(z) max_val &lt;- list() max_val_time &lt;- list() min_val = list() min_val_time = list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y = unname(y) y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) %&gt;% dplyr::mutate(z1 = z_dat) y$time = as.integer(y$time) cluster_data_frames &lt;- vector(&quot;list&quot;, length = K) for (kk in seq(K)) { cluster_mean &lt;- mu[, kk, ] data &lt;- data.frame( X1 = cluster_mean [, 1], X2 = cluster_mean [, 2], X3 = cluster_mean [, 3], time = 1:ntimes ) cluster_data_frames[[kk]] = data } fig &lt;- y %&gt;% plotly::plot_ly( x = ~x, y = ~y, z = ~z, color = ~z1, type = &quot;scatter3d&quot;, frame = ~time, mode = &quot;markers&quot;, size = 80, colors = colorRamp(c(&quot;blue&quot;, &quot;orange&quot;, &quot;red&quot;))) %&gt;% plotly::layout(scene = list( xaxis = list(title = &quot;Diameter&quot;, range = c(1.1* min_val_time[[1]], 1.1 *max_val_time[[1]])), yaxis = list(title = &quot;Chl_Small&quot;, range = c(1.1* min_val_time[[2]], 1.1 *max_val_time[[2]])), zaxis = list(title = &quot;PE&quot;, range = c(1.1* min_val_time[[3]], 1.1 *max_val_time[[3]])), aspectmode = &quot;manual&quot;, # Set aspect ratio to manual aspectratio = list(x = 1, y = 1, z = 1) # Specify the fixed aspect ratio )) updatemenus &lt;- list( list( active = 0, type= &#39;buttons&#39;, buttons = list( list( label = &quot;Data Points&quot;, method = &quot;update&quot;, args = list(list(visible = c(TRUE, rep(c(TRUE, TRUE), K))))), list( label = &quot;No Data Points&quot;, method = &quot;update&quot;, args = list(list(visible = c(FALSE, rep(c(TRUE, TRUE), K)))))) ) ) for (kk in seq(K)) { fig &lt;- fig %&gt;% plotly::add_markers(data = cluster_data_frames[[kk]], x = ~X1, y = ~X2, z = ~X3, color = kk, size = 120, frame = ~time)%&gt;% plotly::layout(updatemenus = updatemenus) } } return(fig) } We used a function from tidyr, so let’s include this package: usethis::use_package(&quot;tidyr&quot;) ## ✔ Adding &#39;tidyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `tidyr::fun()` For now we can use this to visualize the true model, although later this will be useful for visualizing the estimated model. plot_data_and_model(ex1$dat$y, ex1$dat$z, ex1$dat$mu) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu) In 3-d, with a large number of data points, these plots might become too crowded to really appreciate. We therefore have a function that just shows how the cluster centers \\(\\mu_{kt}\\) evolve with time: #&#39; Plot cluster centers in 3-d #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param z a length T list with `z[[t]]` being a n_t vector of cluster assignments #&#39; @param mu a T-by-K-by-d array of means #&#39; @export plot_3d_centers &lt;- function(y, z, mu){ K &lt;- ncol(mu) d &lt;- ncol(y[[1]]) z_dat &lt;- unlist(z) ntimes = length(z) max_val &lt;- list() max_val_time &lt;- list() min_val = list() min_val_time = list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y = unname(y) y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) %&gt;% dplyr::mutate(z1 = z_dat) y$time = as.integer(y$time) cluster_data_frames &lt;- vector(&quot;list&quot;, length = K) for (kk in seq(K)) { cluster_mean &lt;- mu[, kk, ] data &lt;- data.frame( X1 = cluster_mean [, 1], X2 = cluster_mean [, 2], X3 = cluster_mean [, 3], time = 1:ntimes ) cluster_data_frames[[kk]] = data } fig &lt;- plotly::plot_ly() for (kk in seq(K)) { fig &lt;- fig %&gt;% plotly::add_markers(data = cluster_data_frames[[kk]], x = ~X1, y = ~X2, z = ~X3, color = kk, size = 120, frame = ~time) } return(fig) } Let’s try this out for our 3-d example: plot_3d_centers(ex2$dat$y, ex2$dat$z, ex2$dat$mu) There are several other ways of looking at our data and model that might be useful. We would like to see how the \\(\\pi_{kt}\\)’s evolve with time, how a 1-d projection of the 3-d cluster means evolve with time, and how the biomass of a particular cluster evolves over time. Let’s add these functions to our package. #&#39; Plot cluster populations (pi) over time #&#39; @param pi A T-by-K array, with each row consisting of probabilities that sum to one #&#39; @export plot_pi &lt;- function(pi) { # Create an empty data frame df &lt;- data.frame(time = seq_along(pi[, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(pi)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- pi[, k] } # Create the ggplot with multiple line plots pi_plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(pi), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Pi&quot;) + ggplot2::ggtitle(&quot;Pi Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(pi))) # Convert ggplot to plotly for interactivity pi_plotly &lt;- plotly::ggplotly(pi_plt, dynamicTicks = TRUE) return(pi_plotly) } plot_pi(ex2$dat$pi) #&#39; Plot cluster centers as 1-d projection over time #&#39; @param mu a T-by-K-by-d array of means #&#39; @param dim specify which dimension to be plotted: 1 (diam), 2 (chl_small), or 3 (pe) #&#39; @export plot_1d_means &lt;- function(mu, dim = 1) { # Create an empty data frame df &lt;- data.frame(time = seq_along(mu[, 1, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, dim] } # Determine the y-axis label based on the dimension y_label &lt;- switch(dim, &quot;1&quot; = &quot;Diameter&quot;, &quot;2&quot; = &quot;chl_small&quot;, &quot;3&quot; = &quot;pe&quot;, paste(&quot;Dimension&quot;, dim)) # Create the ggplot with multiple line plots pi_plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) # Convert ggplot to plotly for interactivity pi_plotly &lt;- plotly::ggplotly(pi_plt, dynamicTicks = TRUE) return(pi_plotly) } plot_1d_means(ex2$dat$mu, dim = 1) plot_1d_means(ex2$dat$mu, dim = 2) plot_1d_means(ex2$dat$mu, dim = 3) If we want to plot all three dimensions together, we can use the following function: #&#39; Plot cluster means as 1-d projection over time, with all three dimensions plotted together, in separate plots #&#39; @param mu a T-by-K-by-d array of means #&#39; @export plot_1d_means_triple &lt;- function(mu) { # Create an empty data frame for the first plot df1 &lt;- data.frame(time = seq_along(mu[, 1, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df1[[col_name]] &lt;- mu[, k, 1] } # Create the ggplot with multiple line plots for the first plot pi_plt1 &lt;- ggplot2::ggplot(df1, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df1[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Diameter&quot;) + ggplot2::ggtitle(&quot;Means of Diameter Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::guides(size = &quot;none&quot;) # To remove the size legend # Create an empty data frame for the second plot df2 &lt;- data.frame(time = seq_along(mu[, 1, 1])) # Use a for loop to append each column to the data frame for the second plot for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df2[[col_name]] &lt;- mu[, k, 2] } # Create the ggplot with multiple line plots for the second plot pi_plt2 &lt;- ggplot2::ggplot(df2, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df2[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;chl_small&quot;) + ggplot2::ggtitle(&quot;Means of chl_small Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::guides(size = &quot;none&quot;) # To remove the size legend # Create an empty data frame for the third plot df3 &lt;- data.frame(time = seq_along(mu[, 1, 1])) # Use a for loop to append each column to the data frame for the third plot for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df3[[col_name]] &lt;- mu[, k, 3] } # Create the ggplot with multiple line plots for the third plot pi_plt3 &lt;- ggplot2::ggplot(df3, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df3[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;PE&quot;) + ggplot2::ggtitle(&quot;Means of PE Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::guides(size = &quot;none&quot;) # To remove the size legend # Arrange the three plots vertically combined_plot &lt;- gridExtra::grid.arrange(pi_plt1, pi_plt2, pi_plt3, ncol = 1) return(combined_plot) } Here is an example of running this function: plot_1d_means_triple(ex2$dat$mu) ## TableGrob (3 x 1) &quot;arrange&quot;: 3 grobs ## z cells name grob ## 1 1 (1-1,1-1) arrange gtable[layout] ## 2 2 (2-2,1-1) arrange gtable[layout] ## 3 3 (3-3,1-1) arrange gtable[layout] usethis::use_package(&quot;gridExtra&quot;) ## ✔ Adding &#39;gridExtra&#39; to Imports field in DESCRIPTION ## • Refer to functions with `gridExtra::fun()` We now add two more plotting functions to our package: one that plots the 1-d means as above, but with the width of each line varying according to pi of each cluster, and another to plot the total biomass over time for each cluster. #&#39; Plot cluster means as 1-d projection over time, with line widths determined by pi #&#39; @param mu a T-by-K-by-d array of means #&#39; @param pi A T-by-K array, with each row consisting of probabilities that sum to one #&#39; @export plot_1d_means_with_width &lt;- function(mu, pi, dim = 1) { # Create an empty data frame df &lt;- data.frame(time = seq_along(pi[, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, dim] } y_label &lt;- switch(dim, &quot;1&quot; = &quot;Diameter&quot;, &quot;2&quot; = &quot;chl_small&quot;, &quot;3&quot; = &quot;pe&quot;, paste(&quot;Dimension&quot;, dim)) # Create the ggplot with multiple line plots pi_plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k), linewidth = pi[, k]), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::guides(linewidth = &quot;none&quot;) # To remove the linewidth legend return(pi_plt) } plot_1d_means_with_width(ex2$dat$mu, ex2$dat$pi, dim = 3) Let’s also create a function to plot the biomass of each cluster over time. We will run this function later when we have responsibilities defined: #&#39; Plot biomass over time for each cluster #&#39; @param biomass A list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing the biomass (or count) of particles in each bin #&#39; @param resp length T list with `y[[t]]` being a n_t-by-K matrix #&#39; @export plot_biomass = function (biomass, resp){ K &lt;- ncol(resp[[1]]) ntimes = length(resp) df&lt;- data.frame(time = seq_along(resp)) cluster_biomass &lt;- matrix(NA, nrow = ntimes, ncol = K) for (tt in 1:ntimes){ cluster_biomass [tt, ] &lt;- sapply(seq_len(K), function(i) sum(resp[[tt]][, i] * biomass[[tt]])) } # Use a for loop to append each column to the data frame for (k in 1:K) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- cluster_biomass[, k] } # Create the ggplot with multiple line plots pi_plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:K, function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Cluster Biomass&quot;) + ggplot2::ggtitle(&quot;Cluster Biomass Over Time&quot;) #+ # Convert ggplot to plotly for interactivity pi_plotly &lt;- plotly::ggplotly(pi_plt, dynamicTicks = TRUE) return(pi_plotly) } "],["the-method.html", "3 The method 3.1 Initialization 3.2 E-step 3.3 M-step 3.4 Trying out the method", " 3 The method Given a kernel \\(w_h(t)\\) with bandwidth \\(h\\), we will be considering the following EM-inspired algorithm. Here’s a high-level look at the algorithm. We’ll discuss the initialization, E-step, and M-step in the next subsections. #&#39; A Kernel-smoothed EM algorithm for a mixture of Gaussians that change over time #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param hmu bandwidth for mu parameter #&#39; @param hSigma bandwidth for Sigma parameter #&#39; @param hpi bandwidth for pi parameter #&#39; @param num_iter number of iterations of EM to perform #&#39; @param biomass A list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing the biomass (or count) of particles in each bin #&#39; @param initial_fit initial fit from either of the initialization functions (defaults to constant initialization) #&#39; @param times_to_sample number of time points to sample for constant initialization. Used only if initial_fit not given already. #&#39; @param points_to_sample number of bins to sample from each sampled time point for constant initialization. Used only if initial_fit not given already. #&#39; @export kernel_em &lt;- function (y, K, hmu, hSigma, hpi, num_iter = 10, biomass = default_biomass(y), initial_fit = NULL, times_to_sample = 50, points_to_sample = 50){ num_times &lt;- length(y) d &lt;- ncol(y[[1]]) mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) resp &lt;- list() # responsibilities gamma[[t]][i, k] &lt;&lt;initialize-with-mclust&gt;&gt; for (l in seq(num_iter)) { &lt;&lt;E-step&gt;&gt; &lt;&lt;M-step&gt;&gt; } zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) dimnames(Sigma) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL, NULL) dimnames(pi) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K)) fit &lt;- list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) return(fit) } 3.1 Initialization First, we begin with a function that creates a “default biomass” for data that is unbinned (give each point biomass (or count) 1). #&#39; Creates a biomass list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing just 1&#39;s. &lt;&lt;y-param&gt;&gt; default_biomass &lt;- function(y) { biomasslist = vector(&quot;list&quot;, length(y)) for (i in 1:length(y)) { biomasslist[[i]] = as.numeric(rep(1,dim(y[[i]])[1])) } return(biomasslist) } We have tried numerous ways of initializing: randomly sampling points for a constant initialization, fitting a separate mixture model to each time point and trying to match clusters using the Hungarian algorithm, having a “chain of EM-algorithms”, with each one initialized by the previous time; and using a Bayesian approach, which is described below. The methods we settled on are the constant initialization and the Bayesian initialization. The kernel-EM algorithm seems to be able to do about the same with both initializations, and so even though the Bayesian approach gives better results, the speed of the constant initialization makes it the practical choice. 3.1.1 Constant Initialization We get a constant initialization by randomly sampling time points, and then from each sampled time point, randomly sampling some points, and fitting a regular EM-algorithm using the mclust package to the randomly chosen points. #&#39; Initialize the Kernel EM-algorithm using constant parameters #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param times_to_sample number of time points to sample #&#39; @param points_to_sample number of bins to sample from each sampled time point #&#39; @export init_const &lt;- function (y, K, times_to_sample = 50, points_to_sample = 50){ num_times &lt;- length(y) d &lt;- ncol(y[[1]]) mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) sample_data &lt;- matrix(nrow = times_to_sample * points_to_sample, ncol = d) init_fit = NULL # Sample 50 different time points while (is.null(init_fit) == TRUE){ time_points &lt;- sample(1:num_times, times_to_sample, replace = TRUE) row_counter = 1 # Loop over each sampled time point for (tt in time_points) { # Sample 50 rows from the matrix at the current time point rows_to_sample &lt;- sample(1:nrow(y[[tt]]), points_to_sample, replace = TRUE) sampled_rows &lt;- y[[tt]][rows_to_sample, ] # Append the sampled rows to the sample_init matrix sample_data[row_counter:(row_counter + points_to_sample - 1), ] &lt;- sampled_rows row_counter &lt;- row_counter + points_to_sample } if (d == 1){ init_fit &lt;- mclust::Mclust(sample_data, G = K, modelNames = &quot;V&quot;) for (tt in seq(num_times)){ mu[tt, , 1] &lt;- init_fit$parameters$mean Sigma[tt, , 1, 1] &lt;- init_fit$parameters$variance$sigmasq pi [tt, ] &lt;- init_fit$parameters$pro } }else if (d &gt; 1){ init_fit &lt;- mclust::Mclust(sample_data, G = K, modelNames = &quot;VVV&quot;) for (tt in seq(num_times)){ mu[tt, ,] &lt;- t(init_fit$parameters$mean) pi[tt, ] &lt;- init_fit$parameters$pro Sigma[tt, , , ] &lt;- aperm(init_fit$parameters$variance$sigma, c(3,1,2)) } } } #calculate responsibilities (can change this to log densities - numerical stability) resp &lt;- list() # responsibilities gamma[[t]][i, k] if (d == 1) { for (tt in seq(num_times)) { phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu[tt, k, 1], sd = sqrt(Sigma[tt, k, 1, 1])) } temp &lt;- t(t(phi) * pi[tt, ]) resp[[tt]] &lt;- temp / rowSums(temp) } }else if (d &gt; 1){ for (tt in seq(num_times)) { phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu[tt, k, ], sigma = Sigma[tt, k, , ]) } temp &lt;- t(t(phi) * pi[tt, ]) resp[[tt]] &lt;- temp / rowSums(temp) } } zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) fit_init = list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) return(fit_init) } Let’s have a look at what the initialization does on our example data. ex1_init_const = init_const(ex1$dat$y, K = 2) plot_data_and_model(ex1$dat$y, ex1_init_const$zest, ex1_init_const$mu) What about the 3-d initialization? ex2_init_const = init_const(ex2$dat$y, K = 4) plot_data_and_model(ex2$dat$y, ex2_init_const$zest, ex2_init_const$mu) Pretty reasonable centers were chosen! If we get a bad initialization by chance (an unlucky random sampling), we can always re-run the constant initialization as it is very cheap. 3.1.2 Bayesian Initialization To have a more principled initialization that varies with time, we can use a Bayesian approach. We can estimate both \\(\\mu_{tk}\\) and \\(\\hat\\Sigma_{tk}\\) using their posterior means, assuming conjugate prior distributions. Let \\(\\tilde\\gamma_{itk} = C_i^{(t)}\\gamma_{itk}\\), which are “weighted responsibilities”, where each \\(\\gamma_{itk}\\) has been multiplied with the corresponding biomass of bin \\(i\\) at time \\(t\\). We use these weighted responsibilities in our Bayesian approach (can make it unweighted to simplify). For \\(\\pi_{tk}\\), we assume a Dirichlet prior, getting a posterior mean of: (this specific calculation needs to be double checked) \\[ \\hat{\\pi}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right) + \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}} {n_{t-1} + n_{t}} \\] where \\(n_t = \\sum_{i=1}^{B} C_i^{(t)}\\) is the total number of points (or total biomass) at time \\(t\\) (\\(B\\) is the total number of bins (or points, if the data is unbinned)). For \\(\\mu_{tk}\\), we assume a prior that’s a multivariate normal distribution with mean \\(\\mu_{t-1k}\\) and covariance matrix \\(\\frac{1}{n_{t-1k}} \\hat\\Sigma_{t-1k}\\). We then get the following posterior mean: \\[ \\hat{\\mu}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right) \\mu_{kt-1} + \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk} Y_{it}} {\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right) + \\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right)} \\] We then estimate \\(\\hat\\Sigma_{tk}\\) using the posterior mean again, assuming a prior that’s an inverse-Wishart with mean \\(\\hat\\Sigma_{t-1k}\\). Our estimate is: \\[ \\hat{\\Sigma}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right) \\Sigma_{kt-1} + \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk} \\left(Y_{it} - \\mu_{tk}\\right) \\left(Y_{it} - \\mu_{tk}\\right)^T} {\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right) + \\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right)} \\] A bit more thought is required regarding what multiple iterations (default number of iterations is 1) of this Bayesian EM-algorithm would do (on the second iteration and above, we use \\(\\mu_{kt}\\) from the previous iteration in the place of \\(\\mu_{t-1k}\\), and similarly for \\(\\Sigma_{tk}\\) and \\(\\pi_{tk}\\)). We also add the option to introduce Laplace smoothing for calculating the responsibilities, to prevent \\(pi_{tk}\\) collapsing to 0 for some clusters (the default Laplace smoothing constant is 0). Here is the function that implements this approach: #&#39; Initialize the Kernel EM-algorithm using Bayesian methods #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param biomass A list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing the biomass (or count) of particles in each bin #&#39; @param num_iter number of iterations of EM to perform #need to think more about iterations for Bayes init #&#39; @param lap_smooth_const laplace smoothing constant #More explanation needed #&#39; @export init_bayes &lt;- function (y, K, biomass = default_biomass(y), num_iter = 1, lap_smooth_const = 0){ #browser() num_times &lt;- length(y) d &lt;- ncol(y[[1]]) resp &lt;- list() # responsibilities gamma[[t]][i, k] log_resp &lt;- list() resp_weighted &lt;- list() resp_sum &lt;- list() resp_sum_pi &lt;- list() y_sum &lt;- list() mat_sum &lt;- list() yy &lt;- list() mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) if (d == 1){ init_fit &lt;- mclust::Mclust(y[[1]], G = K, modelNames = &quot;V&quot;) ii = 1 while (is.null(init_fit) == TRUE){ init_fit &lt;- mclust::Mclust(y[[1 + ii]], G = K, modelNames = &quot;V&quot;) ii = ii + 1 } mu[1, , 1] &lt;- init_fit$parameters$mean Sigma[1, , 1, 1] &lt;- init_fit$parameters$variance$sigmasq pi [1, ] &lt;- init_fit$parameters$pro } else if (d &gt; 1){ init_fit &lt;- mclust::Mclust(y[[1]], G = K, modelNames = &quot;VVV&quot;) ii = 1 while (is.null(init_fit) == TRUE){ init_fit &lt;- mclust::Mclust(y[[1 + ii]], G = K, modelNames = &quot;V&quot;) ii = ii + 1 } mu[1, ,] &lt;- t(init_fit$parameters$mean) pi[1, ] &lt;- init_fit$parameters$pro Sigma[1, , , ] &lt;- aperm(init_fit$parameters$variance$sigma, c(3,1,2)) } phi &lt;- matrix(NA, nrow(y[[1]]), K) log_phi &lt;- matrix(NA, nrow(y[[1]]), K) if (d == 1) { for (k in seq(K)) { log_phi[, k] &lt;- stats::dnorm(y[[1]], mean = mu[1, k, 1], sd = sqrt(Sigma[1, k, 1, 1]), log = TRUE) } } else if (d &gt; 1) { for (k in seq(K)) { log_phi[, k] &lt;- mvtnorm::dmvnorm(y[[1]], mean = mu[1, k, ], sigma = Sigma[1, k, , ], log = TRUE) } } log_temp = t(t(log_phi) + log(pi[1, ])) log_resp = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[1]] = exp(log_resp) resp_weighted[[1]] = diag(biomass[[1]]) %*% resp[[1]] resp_sum[[1]] &lt;- colSums(resp_weighted[[1]]) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) pi_prev &lt;- pi mu_prev &lt;- mu Sigma_prev &lt;- Sigma for (tt in 2:num_times){ for (l in seq(num_iter)) { # E-step phi &lt;- matrix(NA, nrow(y[[tt]]), K) if (l == 1){ if (d == 1) { for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu_prev[tt - 1, k, 1], sd = sqrt(Sigma_prev[tt - 1, k, 1, 1])) } } else if (d &gt; 1) { for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu_prev[tt - 1, k, ], sigma = Sigma_prev[tt - 1, k, , ]) } } temp &lt;- t(t(phi) * pi_prev[tt - 1, ]) }else if (l &gt; 1) { if (d == 1) { for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu_prev[tt, k, 1], sd = sqrt(Sigma_prev[tt, k, 1, 1])) } } else if (d &gt; 1) { for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu_prev[tt, k, ], sigma = Sigma_prev[tt, k, , ]) } } temp &lt;- t(t(phi) * pi_prev[tt, ]) } temp_smooth = temp + lap_smooth_const * apply(temp, 1, min) resp[[tt]] &lt;- temp_smooth / rowSums(temp_smooth) resp_weighted[[tt]] = diag(biomass[[tt]]) %*% resp[[tt]] zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) #M-step #M-step pi resp_sum[[tt]] &lt;- colSums(resp_weighted[[tt]]) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) for (k in seq(K)){ pi[tt, k] &lt;- (resp_sum [[tt - 1]][, k] + resp_sum [[tt]][, k]) / (rowSums(resp_sum[[tt - 1]]) + rowSums(resp_sum[[tt]])) } #M-step mu y_sum[[tt]] &lt;- crossprod(resp_weighted[[tt]], y[[tt]]) %&gt;% unlist() %&gt;% array(c(K, d)) for (k in seq(K)){ mu[tt, k, ] &lt;- ((resp_sum[[tt - 1]][, k] * mu[tt - 1, k , ]) + y_sum[[tt]][k, ]) / (resp_sum[[tt - 1]] [, k] + resp_sum[[tt]] [, k]) } #M-step Sigma mat_sum[[tt]] &lt;- array(NA, c(K, d, d)) yy[[tt]] &lt;- matrix(NA, dim(y[[tt]])[1], d) for (k in seq(K)) { for(dd in seq(d)) { yy [[tt]][, dd] &lt;- (y[[tt]][, dd]- mu[tt, k, dd]) } mat_sum[[tt]][k, , ] &lt;- crossprod(yy[[tt]], yy[[tt]] * resp_weighted[[tt]][, k]) # YY^T * D * YY } for (k in seq(K)){ Sigma[tt, k, , ] &lt;- ((resp_sum[[tt - 1]][, k] * Sigma[tt - 1, k, , ]) + mat_sum[[tt]][k, , ]) / (resp_sum[[tt - 1]] [, k] + resp_sum[[tt]] [, k]) } pi_prev &lt;- pi mu_prev &lt;- mu Sigma_prev &lt;- Sigma } } dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) dimnames(Sigma) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL, NULL) dimnames(pi) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K)) zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) fit_init = list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) return(fit_init) } Here is what the Bayes initialization gives for the same example data: ex1_init_bayes = init_bayes(ex1$dat$y, K = 2) plot_data_and_model(ex1$dat$y, ex1_init_bayes$zest, ex1_init_bayes$mu) This is much better than the constant initialization! But it will be significantly more computationally expensive with real data. Here is the 3-d example: ex2_init_bayes = init_bayes(ex2$dat$y, K = 4) plot_data_and_model(ex2$dat$y, ex2_init_bayes$zest, ex2_init_bayes$mu) Let’s add the necessary packages to our package: usethis::use_package(&quot;mclust&quot;) usethis::use_package(&quot;matrixStats&quot;) ## ✔ Adding &#39;mclust&#39; to Imports field in DESCRIPTION ## • Refer to functions with `mclust::fun()` ## ✔ Adding &#39;matrixStats&#39; to Imports field in DESCRIPTION ## • Refer to functions with `matrixStats::fun()` Our default initialization for the kernel-em function will be the constant initialization: ###&quot;initialize-with-mclust&quot;### if (is.null(initial_fit)) { initial_fit = init_const(y, K, times_to_sample, points_to_sample) } mu = initial_fit$mu Sigma = initial_fit$Sigma pi = initial_fit$pi Now that we have responsibilities, we can plot the biomass for each cluster over time: plot_biomass(default_biomass(ex2$dat$y), ex2_init_bayes$resp) 3.2 E-step Given an estimate of \\((\\mu,\\Sigma,\\pi)\\), the E-step computes for each \\(Y_{it}\\) how “responsible” each cluster is for it. In particular, the responsibility vector \\((\\hat\\gamma_{it1},\\ldots,\\hat\\gamma_{itK})\\) is a probability vector. It is computed using Bayes rule: \\[ \\hat\\gamma_{itk}=\\hat{\\mathbb{P}}(Z_{it}=k|Y_{it})=\\frac{\\hat \\pi_{tk}\\phi(Y_{it};\\hat\\mu_{tk},\\hat\\Sigma_{tk})}{\\sum_{\\ell=1}^K\\hat \\pi_{t\\ell}\\phi(Y_{it};\\hat\\mu_{t\\ell },\\hat\\Sigma_{t\\ell})} \\] ###&quot;E-step&quot;### # E-step: update responsibilities resp &lt;- list() # responsibilities gamma[[t]][i, k] # E-step: update responsibilities if (d == 1) { for (tt in seq(num_times)) { phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu[tt, k, 1], sd = sqrt(Sigma[tt, k, 1, 1])) } temp &lt;- t(t(phi) * pi[tt, ]) resp[[tt]] &lt;- temp / rowSums(temp) } }else if (d &gt; 1){ for (tt in seq(num_times)) { phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu[tt,k,], sigma = Sigma[tt,k,,]) } temp &lt;- t(t(phi) * pi[tt, ]) resp[[tt]] &lt;- temp / rowSums(temp) } } resp_weighted &lt;- purrr::map2(biomass, resp, ~ .y * .x) 3.3 M-step In the M-step, we update the estimates of \\((\\mu,\\Sigma,\\pi)\\): ###&quot;M-step&quot;### # M-step: update estimates of (mu, Sigma, pi) &lt;&lt;M-step-pi&gt;&gt; &lt;&lt;M-step-mu&gt;&gt; &lt;&lt;M-step-Sigma&gt;&gt; We now assume that we are working with binned data, where \\(C_i^{(t)}\\) is the number of particles (or the biomass) at time \\(t\\) in bin \\(i\\), where \\(i\\) goes from \\(1\\) to \\(B\\), the total number of bins. In the case of unbinned data, we take \\(C_i^{(t)} = 1\\) for each point in the data. \\[ \\hat\\gamma_{\\cdot sk}=\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}, \\] which is an estimate of the number of points (or total biomass) in class \\(k\\) at time \\(s\\), and define \\[ \\tilde\\gamma_{\\cdot tk}=\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}, \\] which is a smoothed version of this estimate. Then we estimate \\(\\pi\\) as follows: \\[ \\hat\\pi_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}}=\\frac{\\tilde\\gamma_{\\cdot tk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}} \\] where \\(n_s = \\sum_{i=1}^{B} C_i^{(s)}\\) is the total number of points (or total biomass) at time \\(s\\). For \\(\\mu\\): \\[ \\hat\\mu_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is}}{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\hat\\gamma_{\\cdot sk}} \\] For \\(\\Sigma\\): \\[ \\hat\\Sigma_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top}{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\hat\\gamma_{\\cdot sk}} \\] Each of these quantities involves a summation over \\(i\\) before the smoothing over time. In each case we do the summation over \\(i\\) first so that then all quantities can be expressed as an array (rather than as lists). This should make the smoothing more efficient. 3.3.1 M-step \\(\\pi\\) For \\(\\pi\\) estimation, we compute \\[ \\hat\\gamma_{\\cdot sk}=\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}, \\] And then we compute the kernel smoothed version of this: \\[ \\tilde\\gamma_{\\cdot tk}=\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}, \\] We are then ready to compute the following: \\[ \\hat\\pi_{tk}=\\frac{\\sum_{s=1}^Tw_h(t-s)\\hat\\gamma_{\\cdot sk}}{\\sum_{s=1}^T{w_h(t-s)n_s}}=\\frac{\\tilde\\gamma_{\\cdot tk}}{\\sum_{s=1}^T{w_h(t-s)n_s}} \\] ###&quot;M-step-pi&quot;### # do summations over i: #form T-by-K matrix summing resp_itk over i resp_sum &lt;- purrr::map(resp_weighted, ~ colSums(.x)) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) resp_sum_smooth &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hpi, x.points = 1:length(x))$y ) pi &lt;- resp_sum_smooth / rowSums(resp_sum_smooth) Here is an example that demonstrates how the ksmooth() function works: xx &lt;- 5 * sin((1:100) / 5) + rnorm(100) plot(xx, type=&quot;o&quot;) lines(stats::ksmooth(1:length(xx), xx, bandwidth = 5, x.points = 1:length(xx)), col=&quot;red&quot;) lines(stats::ksmooth(1:length(xx), xx, bandwidth = 20, x.points = 1:length(xx)), col=&quot;blue&quot;) 3.3.2 M-step \\(\\mu\\) Next, we compute the estimate of \\(\\mu\\). We again first compute the unsmoothed estimate and then apply smoothing to it: \\[ \\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is} \\] This is then used in the following smoothed estimate: \\[ \\hat\\mu_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is}}{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\hat\\gamma_{\\cdot sk}} \\] ###&quot;M-step-mu&quot;### # form T-by-K-by-d array summing resp_itk * Y_ij over i y_sum &lt;- purrr::map2(resp_weighted, y, ~ crossprod(.x, .y)) %&gt;% unlist() %&gt;% array(c(K, d, num_times)) %&gt;% aperm(c(3,1,2)) y_sum_smoothed &lt;- apply( y_sum, 2:3, function(x) stats::ksmooth(1:length(x), x, bandwidth = hmu, x.points = 1:length(x))$y ) resp_sum_smooth_mu &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hmu, x.points = 1:length(x))$y ) for (j in seq(d)) { mu[, , j] &lt;- y_sum_smoothed[, , j] / resp_sum_smooth_mu } In the above code for y_sum, I convert a list of length \\(T\\), where each list element is a \\(K\\times d\\) matrix, to a \\(T\\times K\\times d\\) array. To verify that this conversion is done correctly, I tried this small example: a &lt;- list(matrix(1:12, 4, 3), matrix(13:24, 4, 3)) a a %&gt;% unlist() %&gt;% array(c(4,3,2)) ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 3.3.3 M-step \\(\\Sigma\\) We start by computing \\[ \\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top \\] and then go on to compute \\[ \\hat\\Sigma_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top}{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\hat\\gamma_{\\cdot sk}} \\] ###&quot;M-step-Sigma&quot;### # form a T-by-K-by-d-by-d array # summing (Y_it - mu_t)*diag(resp_itk)*(Y_it - mu_t)^T over i mat_sum &lt;- array(NA, c(num_times, K, d, d)) for (tt in seq(num_times)) { yy &lt;- matrix(NA, dim(y[[tt]])[1], d) for (k in seq(K)) { for(dd in seq(d)) { #yy [,dd] &lt;- (y[[tt]][, dd]- mu_sig[tt, k, dd]) yy [,dd] &lt;- (y[[tt]][, dd]- mu[tt, k, dd]) } mat_sum[tt, k, , ] &lt;- crossprod(yy, yy * resp_weighted[[tt]][, k]) # YY^T * D * YY } } mat_sum_smoothed &lt;- apply( mat_sum, 2:4, function(x) stats::ksmooth(1:length(x), x, bandwidth = hSigma, x.points = 1:length(x))$y ) resp_sum_smooth_Sigma &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hSigma, x.points = 1:length(x))$y ) for (j in seq(d)) for (l in seq(d)) Sigma[, , j, l] &lt;- mat_sum_smoothed[, , j, l] / resp_sum_smooth_Sigma 3.4 Trying out the method Let’s first try out our 1-d example, with all bandwidths equal to 5. Notice that no initialization is explicitly fed into the kernel_em function, so it will use a constant initialization. fit &lt;- kernel_em(ex1$dat$y, K = 2, hmu = 5, hSigma = 5, hpi = 5, num_iter = 20) plot_data_and_model(ex1$dat$y, fit$zest, fit$mu) Now let’s try bandwidths equal to 50. fit &lt;- kernel_em(ex1$dat$y, K = 2, hmu = 50, hSigma = 50, hpi = 50, num_iter = 10, initial_fit = ex1_init_const) plot_data_and_model(ex1$dat$y, fit$zest, fit$mu) Now let’s try out our 3-d example, with bandwidths 5 again: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 5, hSigma = 5, hpi = 5, num_iter = 10, initial_fit = ex2_init_const) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time: plot_pi(fit$pi) plot_1d_means(fit$mu, dim = 1) Let’s have a look at the responsibilities that we get from our model. We consider the first 4 bins in the 50th time point: fit$resp[[50]][1:4, ] ## [,1] [,2] [,3] [,4] ## [1,] 4.862207e-28 1.000000e+00 6.436971e-20 8.095793e-24 ## [2,] 1.687608e-24 1.000000e+00 8.376283e-16 5.476091e-17 ## [3,] 1.000000e+00 1.514772e-31 5.983525e-28 2.678858e-45 ## [4,] 3.306036e-19 1.032597e-20 1.000000e+00 9.304809e-35 We see that the algorithm is pretty certain that the first bin at time point 50 belongs to cluster 2, the second to cluster 2 also, the third to cluster 1, and the 4th to cluster 3. We might not always have this certainty for points that are somewhat “midway” between different cluster centers. Now let’s see what happens when we use a much larger bandwidth: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 50, hSigma = 50, hpi = 50, num_iter = 10, initial_fit = ex2_init_const) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time again: plot_pi(fit$pi) plot_1d_means(fit$mu, dim = 1) Now let’s see what happens when we use the Bayesian initialization: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 5, hSigma = 5, hpi = 5, num_iter = 10, initial_fit = ex2_init_bayes) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time again: plot_pi(fit$pi) plot_1d_means(fit$mu, dim = 1) They look pretty similar! The algorithm is able to make up the difference between the cheap constant initialization and the improved Bayesian initialization, at least in this simple case. "],["conclude.html", "4 Conclusion", " 4 Conclusion When you are done defining the package, it remains to convert the Roxygen to documentation. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowkernel documentation ## ℹ Loading flowkernel ## Writing &#39;NAMESPACE&#39; ## Writing &#39;default_biomass.Rd&#39; ## Writing &#39;flowkernel-package.Rd&#39; ## Writing &#39;generate_smooth_gauss_mix.Rd&#39; ## Writing &#39;init_bayes.Rd&#39; ## Writing &#39;init_const.Rd&#39; ## Writing &#39;kernel_em.Rd&#39; ## Writing &#39;plot_1d_means.Rd&#39; ## Writing &#39;plot_1d_means_triple.Rd&#39; ## Writing &#39;plot_1d_means_with_width.Rd&#39; ## Writing &#39;plot_3d_centers.Rd&#39; ## Writing &#39;plot_data.Rd&#39; ## Writing &#39;plot_data_and_model.Rd&#39; ## Writing &#39;plot_pi.Rd&#39; ## Writing &#39;pipe.Rd&#39; You can also add some extra things to your package here if you like, such as a README, some vignettes, a pkgdown site, etc. See here for an example of how to do this with litr. "],["flowkernel-project.html", "5 flowkernel project", " 5 flowkernel project This repository contains the flowkernel R package, which can be installed as follows: remotes::install_github(&quot;jacobbien/flowkernel-project&quot;, subdir = &quot;flowkernel&quot;) It is implemented using literate programming with litr. Thus, the source code is in the form of a bookdown, available here. Note: Until this repo is made public, the best way to view the bookdown is to first download the repo and then open _bookdown/index.html in a browser locally. To modify the code in this R package, modify the .Rmd files in this directory and then (from an R session in this directory) run the following: litr::render(&quot;index.Rmd&quot;) To install the latest version of litr, run remotes::install_github(&quot;jacobbien/litr-project&quot;, subdir = &quot;litr&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
