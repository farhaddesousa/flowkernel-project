[["index.html", "Creating the flowkernel R package 1 Preliminaries 1.1 DESCRIPTION file 1.2 Package-level documentation", " Creating the flowkernel R package Farhad de Sousa and Jacob Bien 2024-07-15 1 Preliminaries This document uses litr to define the flowkernel R package. When the index.Rmd file is rendered, the R package is created along with the bookdown you are reading. To do so in RStudio, you can simply open index.Rmd and press “Knit” to render the bookdown (and open _book/index.html to see the result). More generally, in a console you can run the following: litr::render(&quot;index.Rmd&quot;) 1.1 DESCRIPTION file We start by specifying some basic information for the description file: usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;Smoothly Varying Mixture of Gaussians Modeling&quot;, Description = &quot;This package uses kernel-smoothed EM to estimate a smoothly varying mixture of Gaussians model.&quot;, `Authors@R` = c(person( given = &quot;Jacob&quot;, family = &quot;Bien&quot;, email = &quot;jbien@usc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ), person( given = &quot;Farhad&quot;, family = &quot;de Sousa&quot;, email = &quot;fdesousa@usc.edu&quot;, role = c(&quot;aut&quot;) ) ) ) ) usethis::use_mit_license(copyright_holder = &quot;F. Last&quot;) 1.2 Package-level documentation Let’s include some package-level documentation. Besides being user-friendly, it’s also needed because we’ll be using “import from” later. Also, notice that we are importing all of mclust. This is because of this error involving mclustBIC(), which is called when we call Mclust(). #&#39; Smoothly Varying Mixture of Gaussians Modeling #&#39; #&#39; This package uses kernel-smoothed EM to estimate a smoothly varying mixture of Gaussians model. #&#39; #&#39; @docType package #&#39; @import mclust "],["the-model.html", "2 The model 2.1 Generating data from model 2.2 Visualizing the raw data 2.3 Visualizing data and model", " 2 The model Our interest is in modeling a sequence of scatter plots measured over time. That is, we observe \\(Y_{it}\\in\\mathbb R^d\\) for \\(i=1,\\ldots,n_t\\) and \\(t=1,\\ldots,T\\). In continuous-time flow cytometry data, we notice that this data has two properties: Each scatter plot looks approximately like a mixture of Gaussians. The general clustering structure seen in each scatter plot is slowly varying over time. To model data like this, we wish to fit a smoothly-varying mixture of Gaussians model: \\[ Y_{it}|\\{Z_{it}=k\\}\\sim N_d(\\mu_{kt},\\Sigma_{kt})\\qquad\\mathbb P(Z_{it}=k)=\\pi_{kt} \\] where \\((\\mu_{kt},\\Sigma_{kt},\\pi_{kt})\\) are slowly varying parameters. The actual flow cytometry data that we will be working with will be binned data - the 3-d space is divided into a grid (\\(Y_{it}\\) will represent the location of the \\(i\\)th bin at time \\(t\\)), and for each bin at each point in time, we have a total carbon biomass - \\(C _i^{(t)}\\), which is estimated based on the number of cells in that grid, and the size of each of those cells. We will be interested in looking at how the total biomass for different populations evolve over time. It will be useful to have data generated from this model for testing purposes, so we begin by defining a function for simulating from this model. For now, we give each point a biomass that is drawn from a normal distribution of mean 0.01 and standard deviation 0.001. 2.1 Generating data from model #&#39; Generate data from smoothly-varying mixture of Gaussians model #&#39; #&#39; The smoothly-varying mixture of Gaussians model is defined as follows: #&#39; #&#39; At time t there are n_t points generated as follows: #&#39; #&#39; Y_{it}|\\{Z_{it}=k\\} ~ N_d(mu_{kt},Sigma_{kt}) #&#39; where #&#39; P(Z_{it}=k)=pi_{kt} #&#39; and the parameters (mu_{kt},Sigma_{kt}, pi_{kt}) are all slowly varying in time. #&#39; #&#39; This function generates Y and Z. #&#39; #&#39; @param mu_function a function that maps a vector of times to a T-by-K-by-d #&#39; array of means #&#39; @param Sigma_function a function that maps a vector of times to a #&#39; T-K-by-d-by-d array of covariance matrices #&#39; @param pi_function a function that maps a vector of times to a T-by-K vector #&#39; of probabilities #&#39; @param num_points a T vector of integers giving the number of points n_t to #&#39; generate at each time point t. #&#39; @export generate_smooth_gauss_mix &lt;- function(mu_function, Sigma_function, pi_function, num_points) { times &lt;- seq_along(num_points) mu &lt;- mu_function(times) Sigma &lt;- Sigma_function(times) pi &lt;- pi_function(times) K &lt;- ncol(pi) # number of components d &lt;- dim(mu)[3] dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) z &lt;- list() # z[[t]][i] = class of point i at time t y &lt;- list() # y[[t]][i,] = d-vector of point i at time t biomass &lt;- list() # biomass[[t]] = biomass of particles in each bin at time t for (t in times) { z[[t]] &lt;- apply(stats::rmultinom(num_points[t], 1, pi[t, ]) == 1, 2, which) y[[t]] &lt;- matrix(NA, num_points[t], d) biomass[[t]] &lt;- abs(rnorm(num_points[t], mean = 0.01, sd = sqrt(0.001))) for (k in 1:K) { ii &lt;- z[[t]] == k # index of points in component k at time t if (sum(ii) == 0) next if (d == 1) y[[t]][ii, ] &lt;- stats::rnorm(n = sum(ii), mean = mu[t, k, ], sd = Sigma[t, k, , ]) else y[[t]][ii, ] &lt;- mvtnorm::rmvnorm(n = sum(ii), mean = mu[t, k, ], sigma = Sigma[t, k, , ]) } } list(y = y, z = z, mu = mu, Sigma = Sigma, pi = pi, biomass = biomass) } For now, we have We have used two packages in this function, so let’s add these into our package. usethis::use_package(&quot;stats&quot;) usethis::use_package(&quot;mvtnorm&quot;) ## ✔ Adding &#39;stats&#39; to Imports field in DESCRIPTION ## • Refer to functions with `stats::fun()` ## ✔ Adding &#39;mvtnorm&#39; to Imports field in DESCRIPTION ## • Refer to functions with `mvtnorm::fun()` Let’s generate simple examples in the \\(d=1\\) and \\(d=3\\) cases: set.seed(123) d &lt;- 1; K &lt;- 2; ntimes &lt;- 200 ex1 &lt;- list( mu_function = function(times) { mu &lt;- array(NA, c(ntimes, K, d)) mu[, , 1] &lt;- cbind(sin(2 * pi * times / 30), 2) mu }, Sigma_function = function(times) { Sigma &lt;- array(NA, c(ntimes, K, 1, 1)) Sigma[, , 1, 1] &lt;- 0.25 Sigma }, pi_function = function(times) { pi1 &lt;- seq(0.2, 0.8, length=length(times)) cbind(pi1, 1 - pi1) }, num_points = rep(40, ntimes) ) ex1$dat &lt;- generate_smooth_gauss_mix(ex1$mu_function, ex1$Sigma_function, ex1$pi_function, ex1$num_points) d &lt;- 3; K &lt;- 4; ntimes &lt;- 200 ex2 = list( mu_function = function(times) { mu &lt;- array(NA, c(ntimes, K, d)) mu[, , 1] &lt;- cbind(0.5*cos(2 * pi * times / 30), 0.3*sin(2 * pi * times / 30), sin(2 * pi * times / 30), -3) mu[, , 2] = cbind (0.3*sin(2 * pi * times / 30), 2, -1, 0.6*cos(2 * pi * times / 30)) mu[, , 3] = cbind(2, 0.7*cos(2 * pi * times / 30), 0.4*sin(2 * pi * times / 30), 1) mu }, Sigma_function = function(times) { Sigma &lt;- array(NA, c(ntimes, K, d, d)) Sigma[, , 1, 1] &lt;- 0.10 Sigma[, , 1, 2] &lt;- 0 Sigma[, , 1, 3] &lt;- 0 Sigma[, , 2, 1] &lt;- 0 Sigma[, , 2, 2] &lt;- 0.10 Sigma[, , 2, 3] &lt;- 0 Sigma[, , 3, 1] &lt;- 0 Sigma[, , 3, 2] &lt;- 0 Sigma[, , 3, 3] &lt;- 0.10 Sigma }, pi_function = function(times) { pi1 &lt;- seq(0.2, 0.3, length=length(times)) cbind(pi1, pi1, 2*pi1/3, 1- (2*pi1 + 2*pi1/3)) }, num_points = rep(150, ntimes) ) ex2$dat &lt;- generate_smooth_gauss_mix(ex2$mu_function, ex2$Sigma_function, ex2$pi_function, ex2$num_points) 2.2 Visualizing the raw data Let’s make a function for visualizing the data in the one-dimensional and three-dimensional cases. library(magrittr) # we&#39;ll be using the pipe in this document The function will take as input the following argument: ###&quot;y-param&quot;### #&#39; @param y length T list with `y[[t]]` being a n_t-by-d matrix We define this bit of documentation in its own code chunk so that it can be easily reused since multiple functions in the package take it as input. #&#39; Plot raw data #&#39; &lt;&lt;y-param&gt;&gt; #&#39; #&#39; @export plot_data &lt;- function(y) { d &lt;- ncol(y[[1]]) if (d == 1){ y_label &lt;- ifelse(is.null(colnames(y[[1]])), &quot;V1&quot;, colnames(y[[1]])) fig &lt;- purrr::map_dfr(y, ~ tibble::tibble(y = .x), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) %&gt;% ggplot2::ggplot(ggplot2::aes(x = .data$time, y = .data$y)) + ggplot2::geom_point(alpha = 0.2) + ggplot2::labs(x = &quot;Time&quot;, y = y_label, title = &quot;Raw Data&quot;) } else if (d == 3){ d &lt;- ncol(y[[1]]) max_val &lt;- list() max_val_time &lt;- list() min_val = list() min_val_time = list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y = unname(y) # Determine axis labels if (is.null(colnames(y[[1]]))) { x_label &lt;- &quot;V1&quot; y_label &lt;- &quot;V2&quot; z_label &lt;- &quot;V3&quot; } else { x_label &lt;- colnames(y[[1]])[1] y_label &lt;- colnames(y[[1]])[2] z_label &lt;- colnames(y[[1]])[3] } y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) y$time = as.integer(y$time) fig &lt;- plotly::plot_ly( data = y, x = ~x, y = ~y, z = ~z, type = &quot;scatter3d&quot;, frame = ~time, mode = &quot;markers&quot;, size = 80, colors = colorRamp(c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;cyan&quot;, &quot;magenta&quot;, &quot;brown&quot;, &quot;gray&quot;, &quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;darkorange&quot;))) %&gt;% plotly::layout(title = &#39;Raw Data&#39;, scene = list( xaxis = list(range = c(1.1 * min_val_time[[1]], 1.1 * max_val_time[[1]]), title = x_label), yaxis = list(range = c(1.1 * min_val_time[[2]], 1.1 * max_val_time[[2]]), title = y_label), zaxis = list(range = c(1.1 * min_val_time[[3]], 1.1 * max_val_time[[3]]), title = z_label), aspectmode = &quot;manual&quot;, aspectratio = list(x = 1, y = 1, z = 1) # Specify the fixed aspect ratio )) } return(fig) } We’ve used some functions from other packages, so let’s include those in our package: usethis::use_pipe() usethis::use_package(&quot;purrr&quot;) usethis::use_package(&quot;tibble&quot;) usethis::use_package(&quot;dplyr&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_import_from(&quot;rlang&quot;, &quot;.data&quot;) usethis::use_package(&quot;plotly&quot;) usethis::use_package(&quot;grDevices&quot;) ## ✔ Adding &#39;magrittr&#39; to Imports field in DESCRIPTION ## ✔ Writing &#39;R/utils-pipe.R&#39; ## • Run `devtools::document()` to update &#39;NAMESPACE&#39; ## ✔ Adding &#39;purrr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `purrr::fun()` ## ✔ Adding &#39;tibble&#39; to Imports field in DESCRIPTION ## • Refer to functions with `tibble::fun()` ## ✔ Adding &#39;dplyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `dplyr::fun()` ## ✔ Adding &#39;ggplot2&#39; to Imports field in DESCRIPTION ## • Refer to functions with `ggplot2::fun()` ## ✔ Adding &#39;rlang&#39; to Imports field in DESCRIPTION ## ✔ Adding &#39;@importFrom rlang .data&#39; to &#39;R/flowkernel-package.R&#39; ## ✔ Writing &#39;NAMESPACE&#39; ## ✔ Adding &#39;plotly&#39; to Imports field in DESCRIPTION ## • Refer to functions with `plotly::fun()` ## ✔ Adding &#39;grDevices&#39; to Imports field in DESCRIPTION ## • Refer to functions with `grDevices::fun()` Let’s look at our two examples using this plotting function: plot_data(ex1$dat$y) plot_data(ex2$dat$y) 2.3 Visualizing data and model We’ll also want a function for plotting the data with points colored by true (or estimated) cluster. And it will be convenient to also be able to superimpose the true (or estimated) means. The next function does this: #&#39; Plot data colored by cluster assignment with cluster means &lt;&lt;y-param&gt;&gt; #&#39; @param z a length T list with `z[[t]]` being a n_t vector of cluster assignments #&#39; @param mu a T-by-K-by-d array of means #&#39; @param dim an integer which specifies which dimension of the data to plot. Defaults to a vector - `c(1:ncol(y[[1]]))` - that will plot all dimensions together #&#39; @param show_data a Boolean variable which determines whether data points are plotted along with the cluster centers or not. Defaults to `TRUE` #&#39; @export plot_data_and_model &lt;- function(y, z, mu, dim = c(1:ncol(y[[1]])), show_data = TRUE) { d &lt;- ncol(y[[1]]) K &lt;- ncol(mu) ntimes &lt;- length(z) if (d == 1) { y_label &lt;- ifelse(is.null(colnames(y[[1]])), paste0(&quot;V&quot;, dim), colnames(y[[1]])[dim]) df &lt;- data.frame(time = seq_along(mu[, 1, 1])) for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, 1] } plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Model Means&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) fig &lt;- plotly::ggplotly(plt, dynamicTicks = TRUE) if (show_data == TRUE) { dat_df &lt;- purrr::map2_dfr(z, y, ~ tibble::tibble(z = as.factor(.x), y = .y), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) means_df &lt;- tibble::as_tibble(mu[, , 1]) %&gt;% dplyr::mutate(time = dplyr::row_number()) %&gt;% tidyr::pivot_longer(-.data$time, names_to = &quot;cluster&quot;, values_to = &quot;Mean&quot;) %&gt;% dplyr::mutate(cluster = as.factor(cluster)) fig &lt;- ggplot2::ggplot() + ggplot2::geom_line( data = means_df, ggplot2::aes(x = .data$time, y = .data$Mean, group = .data$cluster) ) + ggplot2::geom_point( data = dat_df, ggplot2::aes(x = .data$time, y = .data$y, color = .data$z), alpha = 0.2 ) + ggplot2::labs(x = &quot;Time&quot;, y = y_label, title = &quot;Data and Model&quot;) } } else if (d == 3) { z_dat &lt;- unlist(z) if (isTRUE(all.equal(dim, c(1, 2, 3)))) { if (is.null(colnames(y[[1]]))) { x_label &lt;- &quot;V1&quot; y_label &lt;- &quot;V2&quot; z_label &lt;- &quot;V3&quot; } else { x_label &lt;- colnames(y[[1]])[1] y_label &lt;- colnames(y[[1]])[2] z_label &lt;- colnames(y[[1]])[3] } max_val &lt;- list() max_val_time &lt;- list() min_val &lt;- list() min_val_time &lt;- list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) %&gt;% dplyr::mutate(z1 = z_dat) %&gt;% dplyr::mutate(time = as.integer(time)) cluster_data_frames &lt;- vector(&quot;list&quot;, length = K) for (kk in seq(K)) { cluster_mean &lt;- mu[, kk, ] data &lt;- data.frame( X1 = cluster_mean[, 1], X2 = cluster_mean[, 2], X3 = cluster_mean[, 3], time = 1:ntimes ) cluster_data_frames[[kk]] &lt;- data } if (show_data == FALSE) { fig &lt;- plotly::plot_ly(colors = colorRamp(c(&quot;blue&quot;, &quot;orange&quot;, &quot;red&quot;))) %&gt;% plotly::layout(title = &#39;Model Means&#39;) } else { fig &lt;- y %&gt;% plotly::plot_ly( x = ~x, y = ~y, z = ~z, color = ~z1, type = &quot;scatter3d&quot;, frame = ~time, mode = &quot;markers&quot;, size = 80, colors = colorRamp(c(&quot;blue&quot;, &quot;orange&quot;, &quot;red&quot;)) ) %&gt;% plotly::layout(title = &#39;Data and Model&#39;) updatemenus &lt;- list( list( active = 0, type = &#39;buttons&#39;, buttons = list( list( label = &quot;Data Points&quot;, method = &quot;update&quot;, args = list(list(visible = c(TRUE, rep(c(TRUE, TRUE), K)))) ), list( label = &quot;No Data Points&quot;, method = &quot;update&quot;, args = list(list(visible = c(FALSE, rep(c(TRUE, TRUE), K)))) ) ) ) ) } for (kk in seq(K)) { fig &lt;- fig %&gt;% plotly::add_markers(data = cluster_data_frames[[kk]], x = ~X1, y = ~X2, z = ~X3, color = as.factor(kk), size = 120, frame = ~time) if (show_data == TRUE) { fig &lt;- fig %&gt;% plotly::layout(updatemenus = updatemenus) } } fig &lt;- fig %&gt;% plotly::layout(scene = list( xaxis = list(title = x_label, range = c(1.1 * min_val_time[[1]], 1.1 * max_val_time[[1]])), yaxis = list(title = y_label, range = c(1.1 * min_val_time[[2]], 1.1 * max_val_time[[2]])), zaxis = list(title = z_label, range = c(1.1 * min_val_time[[3]], 1.1 * max_val_time[[3]])), aspectmode = &quot;manual&quot;, # Set aspect ratio to manual aspectratio = list(x = 1, y = 1, z = 1) # Specify the fixed aspect ratio )) } else { y &lt;- purrr::map(y, ~ .x[, dim, drop = FALSE]) y_label &lt;- ifelse(is.null(colnames(y[[1]])), paste0(&quot;V&quot;, dim), colnames(y[[1]])[dim]) df &lt;- data.frame(time = seq_along(mu[, 1, 1])) for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, dim] } plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) fig &lt;- plotly::ggplotly(plt, dynamicTicks = TRUE) if (show_data == TRUE) { dat_df &lt;- purrr::map2_dfr(z, y, ~ tibble::tibble(z = as.factor(.x), y = .y), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) means_df &lt;- tibble::as_tibble(mu[, , dim]) %&gt;% dplyr::mutate(time = dplyr::row_number()) %&gt;% tidyr::pivot_longer(-.data$time, names_to = &quot;cluster&quot;, values_to = &quot;Mean&quot;) %&gt;% dplyr::mutate(cluster = as.factor(cluster)) fig &lt;- ggplot2::ggplot() + ggplot2::geom_line( data = means_df, ggplot2::aes(x = .data$time, y = .data$Mean, group = .data$cluster) ) + ggplot2::geom_point( data = dat_df, ggplot2::aes(x = .data$time, y = .data$y, color = .data$z), alpha = 0.2 ) + ggplot2::labs(x = &quot;Time&quot;, y = y_label, title = &quot;Data and Model&quot;) } } } return(fig) } We used a function from tidyr, so let’s include this package: usethis::use_package(&quot;tidyr&quot;) ## ✔ Adding &#39;tidyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `tidyr::fun()` For now we can use this to visualize the true model, although later this will be useful for visualizing the estimated model. We first plot the 1-d data, with and without the data points: plot_data_and_model(ex1$dat$y, ex1$dat$z, ex1$dat$mu) plot_data_and_model(ex1$dat$y, ex1$dat$z, ex1$dat$mu, show_data = FALSE) We next look at the 3-d example. There are a few different ways in which this function can be used. We can plot the entire data with the cluster means and assignments as a 3-d animation, and we can also choose to leave out the data points themselves and just look at how the cluster centers evolve with time (the entire point with the data points can be too crowded with real data): plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, show_data = FALSE) We can also use this function to plot 1-d projections of the 3-d data, with and without the data points: plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, dim = 1, show_data = FALSE) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, dim = 2) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, dim = 2, show_data = FALSE) We would also like to see how the \\(\\pi_{kt}\\)’s evolve with time, and how the biomass of a particular cluster evolves over time. Let’s add these functions to our package. #&#39; Plot cluster populations (pi) over time #&#39; @param pi A T-by-K array, with each row consisting of probabilities that sum to one #&#39; @export plot_pi &lt;- function(pi) { # Create an empty data frame df &lt;- data.frame(time = seq_along(pi[, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(pi)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- pi[, k] } # Create the ggplot with multiple line plots pi_plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(pi), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Pi&quot;) + ggplot2::ggtitle(&quot;Pi Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(pi))) # Convert ggplot to plotly for interactivity fig &lt;- plotly::ggplotly(pi_plt, dynamicTicks = TRUE) return(fig) } plot_pi(ex2$dat$pi) If we want to plot all three dimensions together, we can use the following function: #&#39; Plot cluster means as 1-d projection over time, with all three dimensions plotted together, in separate plots #&#39; @param mu a T-by-K-by-d array of means #&#39; @export plot_1d_means_triple &lt;- function(y, mu) { # Convert the 3D array into a long data frame for ggplot2 long_df &lt;- data.frame( time = rep(seq_along(mu[, 1, 1]), times = ncol(mu) * dim(mu)[3]), value = as.vector(mu), cluster = factor(rep(rep(1:ncol(mu), each = dim(mu)[1]), times = dim(mu)[3])), dimension = factor(rep(rep(1:dim(mu)[3], each = dim(mu)[1] * ncol(mu)), times = 1)) ) if (is.null(colnames(y[[1]]))) { label_1 &lt;- &quot;V1&quot; label_2 &lt;- &quot;V2&quot; label_3 &lt;- &quot;V3&quot; } else { label_1 &lt;- colnames(y[[1]])[1] label_2 &lt;- colnames(y[[1]])[2] label_3 &lt;- colnames(y[[1]])[3] } # Create the labels for the facets facet_labels &lt;- c(&quot;1&quot; = label_1, &quot;2&quot; = label_2, &quot;3&quot; = label_3) # Calculate the range for each dimension range_df &lt;- long_df %&gt;% dplyr::group_by(dimension) %&gt;% dplyr::summarize(min_value = min(value), max_value = max(value)) %&gt;% dplyr::mutate(padding = 0.1 * (max_value - min_value), y_min = min_value - padding, y_max = max_value + padding) # Merge the range information back to the long data frame long_df &lt;- merge(long_df, range_df, by = &quot;dimension&quot;) # Create the ggplot with facet_grid and custom y-limits for each plot plt &lt;- ggplot2::ggplot(long_df, ggplot2::aes(x = time, y = value, color = cluster)) + ggplot2::geom_line() + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Value&quot;) + ggplot2::facet_grid(dimension ~ ., scales = &quot;free_y&quot;, labeller = ggplot2::labeller(dimension = facet_labels)) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::geom_blank(ggplot2::aes(y = y_min)) + # Add blank points for custom y-limits ggplot2::geom_blank(ggplot2::aes(y = y_max)) return(plt) } Here is an example of running this function: plot_1d_means_triple(ex2$dat$y, ex2$dat$mu) We now add two more plotting functions to our package: one that plots the 1-d means as above, but with the width of each line varying according to pi of each cluster, and another to plot the total biomass over time for each cluster. #&#39; Plot cluster means as 1-d projection over time, with line widths determined by pi #&#39; @param mu a T-by-K-by-d array of means #&#39; @param pi A T-by-K array, with each row consisting of probabilities that sum to one #&#39; @export plot_1d_means_with_width &lt;- function(y, mu, pi, dim = 1) { y &lt;- purrr::map(y, ~ .x[, dim, drop = FALSE]) y_label &lt;- ifelse(is.null(colnames(y[[1]])), paste0(&quot;V&quot;, dim), colnames(y[[1]])[dim]) # Create an empty data frame df &lt;- data.frame(time = seq_along(pi[, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, dim] } # Create the ggplot with multiple line plots fig &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k), linewidth = pi[, k]), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::guides(linewidth = &quot;none&quot;) # To remove the linewidth legend return(fig) } plot_1d_means_with_width(ex2$dat$y, ex2$dat$mu, ex2$dat$pi, dim = 1) Let’s also create a function to plot the biomass of each cluster over time. We will run this function later when we have responsibilities defined: #&#39; Plot biomass over time for each cluster #&#39; @param biomass A list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing the biomass (or count) of particles in each bin #&#39; @param resp length T list with `y[[t]]` being a n_t-by-K matrix #&#39; @export plot_biomass &lt;- function(biomass, resp) { K &lt;- ncol(resp[[1]]) ntimes &lt;- length(resp) # Initialize a list to hold data frames for each cluster data_list &lt;- vector(&quot;list&quot;, K) for (k in 1:K) { cluster_biomass &lt;- sapply(1:ntimes, function(tt) sum(resp[[tt]][, k] * biomass[[tt]])) data_list[[k]] &lt;- data.frame(time = seq_along(cluster_biomass), Cluster = paste(&quot;Cluster&quot;, k), Biomass = cluster_biomass) } # Combine all the data frames into one df &lt;- do.call(rbind, data_list) # Create the ggplot with multiple line plots plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time, y = Biomass, color = Cluster)) + ggplot2::geom_line() + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Cluster Biomass&quot;) + ggplot2::ggtitle(&quot;Cluster Biomass Over Time&quot;) # Convert ggplot to plotly for interactivity pi_plotly &lt;- plotly::ggplotly(plt, dynamicTicks = TRUE) return(pi_plotly) } "],["the-method.html", "3 The method 3.1 E-step 3.2 M-step 3.3 Initialization 3.4 Trying out the method", " 3 The method Given a kernel \\(w_h(t)\\) with bandwidth \\(h\\), we will be considering the following EM-inspired algorithm. Here’s a high-level look at the algorithm. We’ll discuss the initialization, E-step, and M-step in the next subsections. #&#39; A Kernel-smoothed EM algorithm for a mixture of Gaussians that changes over time. The larger any of the three bandwidths are, the smoother the corresponding parameter will vary. #&#39; #&#39; Element t in the list `y`, `y[[t]]`, is an n_t-by-d matrix with the coordinates of every point (or bin) at time t given in each row. `biomass[[t]]` is a numeric vector of length n_t, where the ith entry of the vector is the biomass for the bin whose coordinates are given in the ith row of `y[[t]]`. #&#39; #&#39; The initial_fit parameter is a list of initial parameter values that is generated from the initialization functions in the package. It contains initial values at all times for mu, Sigma, and pi, as well as estimates for the responsibilities and cluster membership. Currently, for our cluster membership estimates (z estimate - zest), each bin is assigned to the cluster that is most responsible for it. #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param hmu bandwidth for mu parameter #&#39; @param hSigma bandwidth for Sigma parameter #&#39; @param hpi bandwidth for pi parameter #&#39; @param num_iter number of iterations of EM to perform #&#39; @param biomass list of length T, where each element `biomass[[t]]` is a #&#39; numeric vector of length n_t containing the biomass (or count) of particles #&#39; in each bin #&#39; @param initial_fit a list of starting values for the parameters, responsibilities, and estimated cluster assignments #&#39; @export kernel_em &lt;- function (y, K, hmu, hSigma, hpi, num_iter = 10, biomass = default_biomass(y), initial_fit = init_const(y, K, 50, 50)) { num_times &lt;- length(y) d &lt;- ncol(y[[1]]) mu &lt;- initial_fit$mu Sigma &lt;- initial_fit$Sigma pi &lt;- initial_fit$pi for (l in seq(num_iter)) { &lt;&lt;E-step&gt;&gt; &lt;&lt;M-step&gt;&gt; } zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) dimnames(Sigma) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL, NULL) dimnames(pi) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K)) list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) } To start off our first E-step, we need estimates of \\((\\mu,\\Sigma,\\pi)\\) for every cluster for every time point, which we get from our initialization. Before diving into the initialization, however, let us first look at the main algorithm: 3.1 E-step Given an estimate of \\((\\mu,\\Sigma,\\pi)\\), the E-step computes for each \\(Y_{it}\\) how “responsible” each cluster is for it. In particular, the responsibility vector \\((\\hat\\gamma_{it1},\\ldots,\\hat\\gamma_{itK})\\) is a probability vector. It is computed using Bayes rule: \\[ \\hat\\gamma_{itk}=\\hat{\\mathbb{P}}(Z_{it}=k|Y_{it})=\\frac{\\hat \\pi_{tk}\\phi(Y_{it};\\hat\\mu_{tk},\\hat\\Sigma_{tk})}{\\sum_{\\ell=1}^K\\hat \\pi_{t\\ell}\\phi(Y_{it};\\hat\\mu_{t\\ell },\\hat\\Sigma_{t\\ell})} \\] We will create a function that calculates responsibilities given parameter estimates, as we will need to calculate responsibilities elsewhere too (in the initialization). In the function below, we calculate responsibilities using the log of the densities to help with numerical stability, and we use matrixStats::rowLogSumExps(), which implements the LogSumExp function (also called RealSoftMax) in a stable way. #&#39; Calculates responsibilities for each point (or bin) at each time point, given parameter estimates for all clusters at all times &lt;&lt;y-param&gt;&gt; #&#39; @param mu a T-by-K-by-d array of means #&#39; @param Sigma a T-K-by-d-by-d array of covariance matrices #&#39; @param pi a T-by-K vector of probabilities calculate_responsibilities &lt;- function(y, mu, Sigma, pi){ resp &lt;- list() # responsibilities gamma[[t]][i, k] log_resp &lt;- list() # log of responsibilities d &lt;- ncol(y[[1]]) K &lt;- ncol(mu) num_times &lt;- length(y) if (d == 1) { for (tt in seq(num_times)) { log_phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { log_phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu[tt, k, 1], sd = sqrt(Sigma[tt, k, 1, 1]), log = TRUE) } log_temp = t(t(log_phi) + log(pi[tt, ])) log_resp[[tt]] = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[tt]] = exp(log_resp[[tt]]) } } else if (d &gt; 1) { for (tt in seq(num_times)) { log_phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { log_phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu[tt, k, ], sigma = Sigma[tt, k, , ], log = TRUE) } log_temp = t(t(log_phi) + log(pi[tt, ])) log_resp[[tt]] = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[tt]] = exp(log_resp[[tt]]) } } return(resp) } ###&quot;E-step&quot;### # E-step: update responsibilities resp &lt;- calculate_responsibilities(y, mu, Sigma, pi) resp_weighted &lt;- purrr::map2(biomass, resp, ~ .y * .x) 3.2 M-step In the M-step, we update the estimates of \\((\\mu,\\Sigma,\\pi)\\): ###&quot;M-step&quot;### # M-step: update estimates of (mu, Sigma, pi) &lt;&lt;M-step-pi&gt;&gt; &lt;&lt;M-step-mu&gt;&gt; &lt;&lt;M-step-Sigma&gt;&gt; We now assume that we are working with binned data, where \\(C_i^{(t)}\\) is the number of particles (or the biomass) at time \\(t\\) in bin \\(i\\), where \\(i\\) goes from \\(1\\) to \\(B\\), the total number of bins. In the case of unbinned data, we take \\(C_i^{(t)} = 1\\) for each point in the data. \\[ \\hat\\gamma_{\\cdot sk}=\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}, \\] which is an estimate of the number of points (or total biomass) in class \\(k\\) at time \\(s\\), and define \\[ \\tilde\\gamma_{\\cdot tk}=\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}, \\] which is a smoothed version of this estimate. Then we estimate \\(\\pi\\) as follows: \\[ \\hat\\pi_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}}=\\frac{\\tilde\\gamma_{\\cdot tk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}} \\] where \\(n_s = \\sum_{i=1}^{B} C_i^{(s)}\\) is the total number of points (or total biomass) at time \\(s\\). For \\(\\mu\\): \\[ \\hat\\mu_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is}}{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\hat\\gamma_{\\cdot sk}} \\] For \\(\\Sigma\\): \\[ \\hat\\Sigma_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top}{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\hat\\gamma_{\\cdot sk}} \\] Each of these quantities involves a summation over \\(i\\) before the smoothing over time. In each case we do the summation over \\(i\\) first so that then all quantities can be expressed as an array (rather than as lists). This should make the smoothing more efficient. 3.2.1 M-step \\(\\pi\\) For \\(\\pi\\) estimation, we compute \\[ \\hat\\gamma_{\\cdot sk}=\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}, \\] And then we compute the kernel smoothed version of this: \\[ \\tilde\\gamma_{\\cdot tk}=\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}, \\] We are then ready to compute the following: \\[ \\hat\\pi_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}}=\\frac{\\tilde\\gamma_{\\cdot tk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}} \\] ###&quot;M-step-pi&quot;### # do summations over i: #form T-by-K matrix summing resp_itk over i resp_sum &lt;- purrr::map(resp_weighted, ~ colSums(.x)) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) resp_sum_smooth &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hpi, x.points = 1:length(x))$y ) pi &lt;- resp_sum_smooth / rowSums(resp_sum_smooth) Here is an example that demonstrates how the ksmooth() function works: xx &lt;- 5 * sin((1:100) / 5) + rnorm(100) plot(xx, type=&quot;o&quot;) lines(stats::ksmooth(1:length(xx), xx, bandwidth = 5, x.points = 1:length(xx)), col=&quot;red&quot;) lines(stats::ksmooth(1:length(xx), xx, bandwidth = 20, x.points = 1:length(xx)), col=&quot;blue&quot;) 3.2.2 M-step \\(\\mu\\) Next, we compute the estimate of \\(\\mu\\). We again first compute the unsmoothed estimate and then apply smoothing to it: \\[ \\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is} \\] This is then used in the following smoothed estimate: \\[ \\hat\\mu_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is}}{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\hat\\gamma_{\\cdot sk}} \\] ###&quot;M-step-mu&quot;### # form T-by-K-by-d array summing resp_itk * Y_ij over i y_sum &lt;- purrr::map2(resp_weighted, y, ~ crossprod(.x, .y)) %&gt;% unlist() %&gt;% array(c(K, d, num_times)) %&gt;% aperm(c(3,1,2)) y_sum_smoothed &lt;- apply( y_sum, 2:3, function(x) stats::ksmooth(1:length(x), x, bandwidth = hmu, x.points = 1:length(x))$y ) resp_sum_smooth_mu &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hmu, x.points = 1:length(x))$y ) for (j in seq(d)) { mu[, , j] &lt;- y_sum_smoothed[, , j] / resp_sum_smooth_mu } In the above code for y_sum, I convert a list of length \\(T\\), where each list element is a \\(K\\times d\\) matrix, to a \\(T\\times K\\times d\\) array. To verify that this conversion is done correctly, I tried this small example: a &lt;- list(matrix(1:12, 4, 3), matrix(13:24, 4, 3)) a a %&gt;% unlist() %&gt;% array(c(4,3,2)) ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 3.2.3 M-step \\(\\Sigma\\) We start by computing \\[ \\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top \\] and then go on to compute \\[ \\hat\\Sigma_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top}{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\hat\\gamma_{\\cdot sk}} \\] ###&quot;M-step-Sigma&quot;### # form a T-by-K-by-d-by-d array # summing (Y_it - mu_t)*diag(resp_itk)*(Y_it - mu_t)^T over i mat_sum &lt;- array(NA, c(num_times, K, d, d)) for (tt in seq(num_times)) { yy &lt;- matrix(NA, dim(y[[tt]])[1], d) for (k in seq(K)) { for(dd in seq(d)) { #yy [,dd] &lt;- (y[[tt]][, dd]- mu_sig[tt, k, dd]) yy [,dd] &lt;- (y[[tt]][, dd]- mu[tt, k, dd]) } mat_sum[tt, k, , ] &lt;- crossprod(yy, yy * resp_weighted[[tt]][, k]) # YY^T * D * YY } } mat_sum_smoothed &lt;- apply( mat_sum, 2:4, function(x) stats::ksmooth(1:length(x), x, bandwidth = hSigma, x.points = 1:length(x))$y ) resp_sum_smooth_Sigma &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hSigma, x.points = 1:length(x))$y ) for (j in seq(d)) for (l in seq(d)) Sigma[, , j, l] &lt;- mat_sum_smoothed[, , j, l] / resp_sum_smooth_Sigma 3.3 Initialization First, we begin with a function that creates a “default biomass” for data that is unbinned (we give each point a biomass (or count) of 1). #&#39; Creates a biomass list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing just 1&#39;s. &lt;&lt;y-param&gt;&gt; default_biomass &lt;- function(y) { biomasslist &lt;- vector(&quot;list&quot;, length(y)) for (i in 1:length(y)) { biomasslist[[i]] &lt;- as.numeric(rep(1, dim(y[[i]])[1])) } return(biomasslist) } We have tried numerous ways of initializing: randomly sampling points for a constant initialization, fitting a separate mixture model to each time point and trying to match clusters using the Hungarian algorithm, having a “chain of EM-algorithms”, with each one initialized by the previous time; and using a Bayesian approach, which is described below. The methods we settled on are the constant initialization and the Bayesian initialization. The kernel-EM algorithm seems to be able to do about the same with both initializations, and so even though the Bayesian approach gives better results, the speed of the constant initialization makes it the practical choice. 3.3.1 Constant Initialization We get a constant initialization by randomly sampling time points, and then from each sampled time point, randomly sampling some points, and fitting a regular EM-algorithm using the mclust package to the randomly chosen points. #&#39; Initialize the Kernel EM-algorithm using constant parameters #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param times_to_sample number of time points to sample #&#39; @param points_to_sample number of bins to sample from each sampled time point #&#39; @export init_const &lt;- function (y, K, times_to_sample = 50, points_to_sample = 50){ num_times &lt;- length(y) d &lt;- ncol(y[[1]]) mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) # subsample data: times_to_sample &lt;- sample(num_times, times_to_sample, replace=TRUE) if (d == 1) { sample_data &lt;- y[times_to_sample] %&gt;% purrr::map(~ .x[sample(nrow(.x), points_to_sample, replace=TRUE)]) %&gt;% unlist() } else { sample_data &lt;- y[times_to_sample] %&gt;% purrr::map(~ t(.x[sample(nrow(.x), points_to_sample, replace=TRUE), ])) %&gt;% unlist() %&gt;% matrix(ncol = d, byrow = TRUE) } # Repeatedly call Mclust until it gives a non-NULL fit: init_fit &lt;- NULL while (is.null(init_fit)) { if (d == 1) { init_fit &lt;- mclust::Mclust(sample_data, G = K, modelNames = &quot;V&quot;) for (tt in seq(num_times)) { mu[tt, , 1] &lt;- init_fit$parameters$mean Sigma[tt, , 1, 1] &lt;- init_fit$parameters$variance$sigmasq pi[tt, ] &lt;- init_fit$parameters$pro } } else if (d &gt; 1) { init_fit &lt;- mclust::Mclust(sample_data, G = K, modelNames = &quot;VVV&quot;) for (tt in seq(num_times)) { mu[tt, ,] &lt;- t(init_fit$parameters$mean) pi[tt, ] &lt;- init_fit$parameters$pro Sigma[tt, , , ] &lt;- aperm(init_fit$parameters$variance$sigma, c(3,1,2)) } } } #calculate responsibilities resp &lt;- calculate_responsibilities(y, mu, Sigma, pi) zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) } In the above, we concatenated a list of matrices. Here’s a small example to verify that this is working as intended: list_of_mats &lt;- list(matrix(1:10, 2, 5), matrix(11:25, 3, 5)) list_of_mats list(list_of_mats) %&gt;% purrr::map(~ t(.x)) %&gt;% unlist() %&gt;% matrix(ncol = 5, byrow = TRUE) ## [[1]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 ## ## [[2]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 14 17 20 23 ## [2,] 12 15 18 21 24 ## [3,] 13 16 19 22 25 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 ## [5,] 21 22 23 24 25 Let’s have a look at what the initialization does on our example data. ex1_init_const = init_const(ex1$dat$y, K = 2) plot_data_and_model(ex1$dat$y, ex1_init_const$zest, ex1_init_const$mu) What about the 3-d initialization? ex2_init_const = init_const(ex2$dat$y, K = 4) plot_data_and_model(ex2$dat$y, ex2_init_const$zest, ex2_init_const$mu) Pretty reasonable centers were chosen! If we get a bad initialization by chance (an unlucky random sampling), we can always re-run the constant initialization as it is very cheap. 3.3.2 Bayesian Initialization We’d like to have an initialization method that can vary with time yet can do well even for time points where there is only a small number of particles in a given cluster. The idea is to take a Bayesian approach in which the previous time point serves as the prior for the next time point. The implicit assumption is that the parameters vary sufficiently smoothly that the previous time point’s parameter values are a reasonable estimate of the next time point’s parameter values. For now, we take the priors to be based on the previous time point, but we can also consider priors formed based on all previous time points. Let \\(\\tilde\\gamma_{itk} = C_i^{(t)}\\gamma_{itk}\\), which are “weighted responsibilities”, where each \\(\\gamma_{itk}\\) has been multiplied with the corresponding biomass of bin \\(i\\) at time \\(t\\). We use these weighted responsibilities in our Bayesian approach (can make it unweighted to simplify). 3.3.2.1 M-step \\(\\pi\\) For \\(\\pi_{tk}\\), we assume a Dirichlet prior, getting a posterior mean of: (this specific calculation needs to be double checked) At time \\(t\\) we imagine estimating the number of points in each class \\(k\\), which is \\[ n_t\\hat{\\pi}_{tk}|\\pi_{tk}\\sim\\text{Multinomial}(n_t,\\pi_{t}) \\] where \\(\\hat{\\pi}_{tk},\\pi_{tk}\\) are \\(K\\)-dimensional probability vectors. Our prior takes the previous number of points \\[ \\pi_{tk}\\sim \\text{Dirichlet}(n_{t-1}\\hat{\\pi}_{t-1k}). \\] (Here we are conditioning on everything that happened before time \\(t\\), but omitting this in the notation.) Thus, the posterior is \\[ \\pi_{tk}|n_t\\hat\\pi_{tk}\\sim \\text{Dirichlet}(n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1}) \\] and the posterior mean is thus \\[ \\mathbb{E}[\\pi_{tk}|n_t\\hat\\pi_{tk}]=\\frac{n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1,k}}{\\sum_{k=1}^K[n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1,k}]}=\\frac{n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1,k}}{n_t+n_{t-1}}. \\] In other words, we estimate the vector \\(\\pi_{tk}\\) to be the convex combination \\(\\alpha_t\\hat\\pi_{tk}+(1-\\alpha_t)\\hat\\pi_{t-1k}\\), where \\(\\alpha_t=n_t/(n_t+n_{t-1})\\). Our final estimate is therefore \\[ \\hat{\\pi}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right) + \\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right)} {n_{t-1} + n_{t}} \\] where \\(n_t = \\sum_{i=1}^{B} C_i^{(t)}\\) is the total number of points (or total biomass) at time \\(t\\). Here, \\(B\\) is the total number of bins (or points, if the data is unbinned). 3.3.2.2 M-step \\(\\mu\\) For estimating \\(\\mu_{tk}\\), we assume that we known covariance matrix at time \\(t\\): \\(\\hat\\Sigma_{t-1,k}\\) (the previous time’s estimate); and we assume a multivariate normal prior centered around the previous time’s mean, with the scaled covariance matrix: \\[ \\mu_{tk}\\sim\\mathcal{N}(\\hat{\\mu}_{t-1,k}, \\frac{1}{n_{t-1,k}} \\hat\\Sigma_{t-1,k}) \\] We then get the following posterior: \\[ \\mu_{tk}|\\hat{\\mu}_{tk}\\sim\\mathcal{N}(\\frac{n_{tk}\\bar{Y}_{it} + n_{t-1}\\hat{\\mu}_{t-1,k}}{n_{tk}+n_{t-1,k}}, \\frac{1}{n_{t-1,k} + n_{tk}} \\hat\\Sigma_{t-1,k}) \\] where \\(\\bar{Y}_{it} = \\frac{\\sum_{i=1}^{n_{t}} \\tilde\\gamma_{itk}Y_{it}}{\\sum_{i=1}^{n_{t}} \\tilde\\gamma_{itk}}\\) can be thought of as the mean of our data points in cluster \\(k\\) at time \\(t\\). Our estimate for \\(\\mu_{tk}\\) is then \\[ \\hat{\\mu}_{tk} = \\mathbb{E}[\\mu_{tk}|\\hat\\mu_{tk}] = \\frac{ \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk} Y_{it} + \\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right) \\hat{\\mu}_{t-1,k}} { \\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right) + \\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right)} \\] 3.3.2.3 M-step \\(\\Sigma\\) We then estimate \\(\\hat\\Sigma_{tk}\\) in a similar way, assuming a prior that’s an inverse-Wishart with mean \\(\\hat\\Sigma_{t-1,k}\\). Our estimate is: \\[ \\hat{\\Sigma}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right) \\Sigma_{k,t-1} + \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk} \\left(Y_{it} - \\mu_{tk}\\right) \\left(Y_{it} - \\mu_{tk}\\right)^T} {\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right) + \\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right)} \\] A bit more thought is required regarding what multiple iterations (default number of iterations is 1) of this Bayesian EM-algorithm would do (on the second iteration and above, we use \\(\\mu_{kt}\\) from the previous iteration in the place of \\(\\mu_{t-1,k}\\), and similarly for \\(\\Sigma_{tk}\\) and \\(\\pi_{tk}\\)). We also add the option to introduce Laplace smoothing for calculating the responsibilities, to prevent \\(\\pi_{tk}\\) collapsing to 0 over time for some clusters. The Laplace smoothing works by replacing the responsibilities calculation with: \\[ \\hat\\gamma_{itk}=\\hat{\\mathbb{P}}(Z_{it}=k|Y_{it})=\\frac{\\hat \\pi_{tk}\\phi(Y_{it};\\hat\\mu_{tk},\\hat\\Sigma_{tk}) + \\alpha \\min_{c} \\hat \\pi_{tc}\\phi(Y_{it};\\hat\\mu_{tc},\\hat\\Sigma_{tc})}{\\sum_{\\ell=1}^K \\left( \\hat \\pi_{t\\ell}\\phi(Y_{it};\\hat\\mu_{t\\ell },\\hat\\Sigma_{t\\ell}) + \\alpha \\min_{c} \\hat \\pi_{tc}\\phi(Y_{it};\\hat\\mu_{tc},\\hat\\Sigma_{tc}) \\right)} \\] where \\(\\alpha_L\\) is the Laplace smoothing constant (which defaults to 0). Here is the function that implements this approach: #&#39; Initialize the Kernel EM-algorithm using Bayesian methods #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param biomass list of length T, where each element `biomass[[t]]` is a #&#39; numeric vector of length n_t containing the biomass (or count) of particles #&#39; in each bin #&#39; @param num_iter number of iterations of EM to perform #need to think more #&#39; about iterations for Bayes init #&#39; @param lap_smooth_const Laplace smoothing constant #More explanation needed #&#39; @export init_bayes &lt;- function (y, K, biomass = default_biomass(y), num_iter = 1, lap_smooth_const = 0){ num_times &lt;- length(y) d &lt;- ncol(y[[1]]) resp &lt;- list() # responsibilities gamma[[t]][i, k] log_resp &lt;- list() resp_weighted &lt;- list() resp_sum &lt;- list() resp_sum_pi &lt;- list() y_sum &lt;- list() mat_sum &lt;- list() yy &lt;- list() mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) if (d == 1){ init_fit &lt;- mclust::Mclust(y[[1]], G = K, modelNames = &quot;V&quot;) ii = 1 while (is.null(init_fit) == TRUE){ init_fit &lt;- mclust::Mclust(y[[1 + ii]], G = K, modelNames = &quot;V&quot;) ii = ii + 1 } mu[1, , 1] &lt;- init_fit$parameters$mean Sigma[1, , 1, 1] &lt;- init_fit$parameters$variance$sigmasq pi [1, ] &lt;- init_fit$parameters$pro } else if (d &gt; 1){ init_fit &lt;- mclust::Mclust(y[[1]], G = K, modelNames = &quot;VVV&quot;) ii = 1 while (is.null(init_fit) == TRUE){ init_fit &lt;- mclust::Mclust(y[[1 + ii]], G = K, modelNames = &quot;V&quot;) ii = ii + 1 } mu[1, ,] &lt;- t(init_fit$parameters$mean) pi[1, ] &lt;- init_fit$parameters$pro Sigma[1, , , ] &lt;- aperm(init_fit$parameters$variance$sigma, c(3,1,2)) } phi &lt;- matrix(NA, nrow(y[[1]]), K) log_phi &lt;- matrix(NA, nrow(y[[1]]), K) if (d == 1) { for (k in seq(K)) { log_phi[, k] &lt;- stats::dnorm(y[[1]], mean = mu[1, k, 1], sd = sqrt(Sigma[1, k, 1, 1]), log = TRUE) } } else if (d &gt; 1) { for (k in seq(K)) { log_phi[, k] &lt;- mvtnorm::dmvnorm(y[[1]], mean = mu[1, k, ], sigma = Sigma[1, k, , ], log = TRUE) } } log_temp = t(t(log_phi) + log(pi[1, ])) log_resp = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[1]] = exp(log_resp) resp_weighted[[1]] = diag(biomass[[1]]) %*% resp[[1]] resp_sum[[1]] &lt;- colSums(resp_weighted[[1]]) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) pi_prev &lt;- pi mu_prev &lt;- mu Sigma_prev &lt;- Sigma for (tt in 2:num_times){ for (l in seq(num_iter)) { # E-step phi &lt;- matrix(NA, nrow(y[[tt]]), K) if (l == 1){ if (d == 1) { for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu_prev[tt - 1, k, 1], sd = sqrt(Sigma_prev[tt - 1, k, 1, 1])) } } else if (d &gt; 1) { for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu_prev[tt - 1, k, ], sigma = Sigma_prev[tt - 1, k, , ]) } } temp &lt;- t(t(phi) * pi_prev[tt - 1, ]) }else if (l &gt; 1) { if (d == 1) { for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu_prev[tt, k, 1], sd = sqrt(Sigma_prev[tt, k, 1, 1])) } } else if (d &gt; 1) { for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu_prev[tt, k, ], sigma = Sigma_prev[tt, k, , ]) } } temp &lt;- t(t(phi) * pi_prev[tt, ]) } temp_smooth = temp + lap_smooth_const * apply(temp, 1, min) resp[[tt]] &lt;- temp_smooth / rowSums(temp_smooth) resp_weighted[[tt]] = diag(biomass[[tt]]) %*% resp[[tt]] zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) #M-step #M-step pi resp_sum[[tt]] &lt;- colSums(resp_weighted[[tt]]) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) for (k in seq(K)){ pi[tt, k] &lt;- (resp_sum [[tt - 1]][, k] + resp_sum [[tt]][, k]) / (rowSums(resp_sum[[tt - 1]]) + rowSums(resp_sum[[tt]])) } #M-step mu y_sum[[tt]] &lt;- crossprod(resp_weighted[[tt]], y[[tt]]) %&gt;% unlist() %&gt;% array(c(K, d)) for (k in seq(K)){ mu[tt, k, ] &lt;- ((resp_sum[[tt - 1]][, k] * mu[tt - 1, k , ]) + y_sum[[tt]][k, ]) / (resp_sum[[tt - 1]][, k] + resp_sum[[tt]][, k]) } #M-step Sigma mat_sum[[tt]] &lt;- array(NA, c(K, d, d)) yy[[tt]] &lt;- matrix(NA, dim(y[[tt]])[1], d) for (k in seq(K)) { for(dd in seq(d)) { yy [[tt]][, dd] &lt;- (y[[tt]][, dd]- mu[tt, k, dd]) } mat_sum[[tt]][k, , ] &lt;- crossprod(yy[[tt]], yy[[tt]] * resp_weighted[[tt]][, k]) # YY^T * D * YY } for (k in seq(K)){ Sigma[tt, k, , ] &lt;- ((resp_sum[[tt - 1]][, k] * Sigma[tt - 1, k, , ]) + mat_sum[[tt]][k, , ]) / (resp_sum[[tt - 1]] [, k] + resp_sum[[tt]] [, k]) } pi_prev &lt;- pi mu_prev &lt;- mu Sigma_prev &lt;- Sigma } } dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) dimnames(Sigma) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL, NULL) dimnames(pi) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K)) zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) fit_init = list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) return(fit_init) } Here is what the Bayes initialization gives for the same example data: ex1_init_bayes = init_bayes(ex1$dat$y, K = 2) plot_data_and_model(ex1$dat$y, ex1_init_bayes$zest, ex1_init_bayes$mu) This is much better than the constant initialization! But it will be significantly more computationally expensive with real data. Here is the 3-d example: ex2_init_bayes = init_bayes(ex2$dat$y, K = 4) plot_data_and_model(ex2$dat$y, ex2_init_bayes$zest, ex2_init_bayes$mu) Let’s add the necessary packages to our package: usethis::use_package(&quot;mclust&quot;) usethis::use_package(&quot;matrixStats&quot;) ## ✔ Adding &#39;mclust&#39; to Imports field in DESCRIPTION ## • Refer to functions with `mclust::fun()` ## ✔ Adding &#39;matrixStats&#39; to Imports field in DESCRIPTION ## • Refer to functions with `matrixStats::fun()` Now that we have responsibilities, we can plot the biomass for each cluster over time: plot_biomass(ex2$dat$biomass, ex2_init_bayes$resp) The lines are so jagged because we independently drew from a normal distribution for the biomass of each “bin” at each time. 3.4 Trying out the method Let’s first try out our 1-d example, with all bandwidths equal to 5. Notice that no initialization is explicitly fed into the kernel_em() function, so it will use a constant initialization. fit &lt;- kernel_em(ex1$dat$y, K = 2, hmu = 5, hSigma = 5, hpi = 5, num_iter = 20) plot_data_and_model(ex1$dat$y, fit$zest, fit$mu) Now let’s try bandwidths equal to 50. fit &lt;- kernel_em(ex1$dat$y, K = 2, hmu = 50, hSigma = 50, hpi = 50, num_iter = 10, initial_fit = ex1_init_const) plot_data_and_model(ex1$dat$y, fit$zest, fit$mu) Now let’s try out our 3-d example, with bandwidths 5 again: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 5, hSigma = 5, hpi = 5, num_iter = 10, initial_fit = ex2_init_const) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time: plot_pi(fit$pi) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 1, show_data = FALSE) We can look at how the means evolve in any dimension with the data too: plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 2) Let’s have a look at the responsibilities that we get from our model. We consider the first 4 bins in the 50th time point: fit$resp[[50]][1:4, ] ## [,1] [,2] [,3] [,4] ## [1,] 2.284311e-08 1.283566e-16 2.844005e-24 1.000000e+00 ## [2,] 8.359285e-12 6.579083e-19 3.333286e-26 1.000000e+00 ## [3,] 8.566423e-13 1.000000e+00 1.673301e-34 5.070375e-23 ## [4,] 3.618382e-20 3.914394e-29 1.000000e+00 9.759917e-24 We see that the algorithm is pretty certain tabout it’s assignments, but we might not always have this certainty for points that are somewhat “midway” between different cluster centers. For example: round(fit$resp[[92]][135,],2) ## [1] 0 0 0 1 Now let’s see what happens when we use a much larger bandwidth: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 50, hSigma = 50, hpi = 50, num_iter = 10, initial_fit = ex2_init_const) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time again: plot_pi(fit$pi) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 1, show_data = FALSE) Now let’s see what happens when we use the Bayesian initialization: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 5, hSigma = 5, hpi = 5, num_iter = 10, initial_fit = ex2_init_bayes) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time again: plot_pi(fit$pi) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 1, show_data = FALSE) They look pretty similar! The algorithm is able to make up the difference between the cheap constant initialization and the improved Bayesian initialization, at least in this simple case. "],["conclude.html", "4 Conclusion", " 4 Conclusion When you are done defining the package, it remains to convert the Roxygen to documentation. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowkernel documentation ## ℹ Loading flowkernel ## Writing &#39;NAMESPACE&#39; ## Writing &#39;calculate_responsibilities.Rd&#39; ## Writing &#39;default_biomass.Rd&#39; ## Writing &#39;flowkernel-package.Rd&#39; ## Writing &#39;generate_smooth_gauss_mix.Rd&#39; ## Writing &#39;init_bayes.Rd&#39; ## Writing &#39;init_const.Rd&#39; ## Writing &#39;kernel_em.Rd&#39; ## Writing &#39;plot_1d_means_triple.Rd&#39; ## Writing &#39;plot_1d_means_with_width.Rd&#39; ## Writing &#39;plot_biomass.Rd&#39; ## Writing &#39;plot_data.Rd&#39; ## Writing &#39;plot_data_and_model.Rd&#39; ## Writing &#39;plot_pi.Rd&#39; ## Writing &#39;pipe.Rd&#39; You can also add some extra things to your package here if you like, such as a README, some vignettes, a pkgdown site, etc. See here for an example of how to do this with litr. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
